{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrap_utils import url_generator, Collaboration_Graph_Scraper, download_pdf\n",
    "\n",
    "author_ls = ['basu_a']\n",
    "category_ls = ['cs.LG', 'cs.AI', 'math.CO', 'stat.ML']\n",
    "query = url_generator(author_ls=author_ls, category_ls=category_ls, max_results=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser\n",
    "\n",
    "feed = feedparser.parse(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "\n",
    "# Open a PDF document\n",
    "pdf_document = \"../data/pdf/2105.14835v5.pdf\"\n",
    "doc = fitz.open(pdf_document)\n",
    "\n",
    "# Extract text from each page\n",
    "pages = [doc[i].get_text(\"text\") for i in range(len(doc))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper = ''.join(pages).replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arXiv:2105.14835v5  [cs.LG]  17 Jul 2024 Towards Lower Bounds on the Depth of ReLU Neural Networks ∗ Christoph Hertrich1, Amitabh Basu2, Marco Di Summa3, and Martin Skutella4 1London School of Economics and Political Science, London, UK, c.hertrich@lse.ac.uk 2Johns Hopkins University, Baltimore, USA, basu.amitabh@jhu.edu 3Universit`a degli Studi di Padova, Padua, Italy, disumma@math.unipd.it 4Technische Universit¨at Berlin, Berlin, Germany, martin.skutella@tu-berlin.de Abstract We contribute to a better understanding of the class of functions that can be represented by a neural network with ReLU activations and a given architecture. Using techniques from mixed-integer optimization, polyhedral theory, and tropical geometry, we provide a mathemati- cal counterbalance to the universal approximation theorems which suggest that a single hidden layer is suﬃcient for learning any function. In particular, we investigate whether the class of exactly representable functions strictly increases by adding more layers (with no restrictions on size). As a by-product of our investigations, we settle an old conjecture about piecewise linear functions by Wang and Sun [74] in the aﬃrmative. We also present upper bounds on the sizes of neural networks required to represent functions with logarithmic depth. 1 Introduction A core problem in machine learning and statistics is the estimation of an unknown data distribution with access to independent and identically distributed samples from the distribution. It is well- known that there is a tension between the expressivity of the model chosen to approximate the distribution and the number of samples needed to solve the problem with high conﬁdence (or equivalently, the variance one has in one’s estimate). This is referred to as the bias-variance trade- oﬀor the bias-complexity trade-oﬀ. Neural networks provide a way to turn this bias-complexity ∗Authors’ accepted manuscript; to appear in the SIAM Journal on Discrete Mathematics. A preliminary conference version appeared in the proceedings of the NeurIPS 2021 conference. We thank the anonymous referees of both the journal and the conference version for their insightful comments which helped to improve the presentation and clarity. Christoph Hertrich gratefully acknowledges funding by DFG-GRK 2434 “Facets of Complexity”. Amitabh Basu gratefully acknowledges support from AFOSR Grant FA95502010341 and NSF Grant CCF2006587. Martin Skutella gratefully acknowledges funding by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany’s Excellence Strategy — The Berlin Mathematics Research Center MATH+ (EXC-2046/1, project ID: 390685689). 1 knob in a controlled manner that has been studied for decades going back to the idea of a perceptron by Rosenblatt [62]. This is done by modifying the architecture of a neural network class of functions, in particular its size in terms of depth and width. As one increases these parameters, the class of functions becomes more expressive. In terms of the bias-variance trade-oﬀ, the “bias” decreases as the class of functions becomes more expressive, but the “variance” or “complexity” increases. So-called universal approximation theorems [5,18,40] show that even with a single hidden layer, that is, when the depth of the architecture achieves its smallest possible value, one can essentially reduce the “bias” as much as one desires, by increasing the width. Nevertheless, it can be advan- tageous both theoretically and empirically to increase the depth because a substantial reduction in the size can be achieved by this [6,21,46,63,69,70,75]. To get a better quantitative handle on these trade-oﬀs, it is important to understand what classes of functions are exactly representable by neural networks with a certain architecture. The precise mathematical statements of universal approximation theorems show that single layer networks can arbitrarily well approximate any con- tinuous function (under some additional mild hypotheses). While this suggests that single layer networks are good enough from a learning perspective, from a mathematical perspective, one can ask the question if the class of functions represented by single layer networks is a strict subset of the class of functions represented by networks with two or more hidden layers. On the question of size, one can ask for precise bounds on the required width of a network with given depth to represent a certain class of functions. A better understanding of the function classes exactly represented by diﬀerent architectures has implications not just for mathematical foundations, but also algorithmic and statistical learning aspects of neural networks, as recent advances on the training complexity show [6, 11, 23, 28, 42]. The task of searching for the “best” function in a class can only beneﬁt from a better understanding of the nature of functions in that class. A motivating question behind the results in this paper is to understand the hierarchy of function classes exactly represented by neural networks of increasing depth. We now introduce more precise notation and terminology to set the stage for our investigations. 1.1 Notation and Deﬁnitions We write [n] := {1, 2, . . . , n} for the set of natural numbers up to n (without zero) and [n]0 := [n] ∪{0} for the same set including zero. For any n ∈N, let σ: Rn →Rn be the component-wise rectiﬁer function σ(x) = (max{0, x1}, max{0, x2}, . . . , max{0, xn}). For any number of hidden layers k ∈N, a (k +1)-layer feedforward neural network with rectiﬁed linear units (ReLU NN or simply NN) is given by k aﬃne transformations T (ℓ) : Rnℓ−1 →Rnℓ, x 7→A(ℓ)x + b(ℓ), for ℓ∈[k], and a linear transformation T (k+1) : Rnk →Rnk+1, x 7→A(k+1)x. It is said to compute or represent the function f : Rn0 →Rnk+1 given by f = T (k+1) ◦σ ◦T (k) ◦σ ◦· · · ◦T (2) ◦σ ◦T (1). The matrices A(ℓ) ∈Rnℓ×nℓ−1 are called the weights and the vectors b(ℓ) ∈Rnℓare the biases of the ℓ-th layer. The number nℓ∈N is called the width of the ℓ-th layer. The maximum width of all hidden layers maxℓ∈[k] nℓis called the width of the NN. Further, we say that the NN has depth k + 1 and size Pk ℓ=1 nℓ. Often, NNs are represented as layered, directed, acyclic graphs where each dimension of each layer (including input layer ℓ= 0 and output layer ℓ= k + 1) is one vertex, weights are arc labels, and biases are node labels. Then, the vertices are called neurons. 2 x1 x2 y 1 1 -1 1 -1 1 -1 Figure 1: An NN with two input neurons, labeled x1 and x2, three hidden neurons, labeled with the shape of the rectiﬁer function, and one output neuron, labeled y. The arcs are labeled with their weights and all biases are zero. The NN has depth 2, width 3, and size 3. It computes the function x 7→y = max{0, x1 −x2} + max{0, x2} −max{0, −x2} = max{0, x1 −x2} + x2 = max{x1, x2}. For a given input x = x(0) ∈Rn0, let y(ℓ) := T (ℓ)(x(ℓ−1)) ∈Rnℓbe the activation vector and x(ℓ) := σ(yℓ) ∈Rnℓthe output vector of the ℓ-th layer. Further, let y := y(k+1) = f(x) be the output of the NN. We also say that the i-th component of each of these vectors is the activation or the output of the i-th neuron in the ℓ-th layer. To illustrate the deﬁnition of NNs and how they compute functions, Figure 1 shows an NN with one hidden layer computing the maximum of two numbers. For k ∈N, we deﬁne ReLUn(k) := {f : Rn →R | f can be represented by a (k + 1)-layer NN}, CPWLn := {f : Rn →R | f is continuous and piecewise linear}. By deﬁnition, a continuous function f : Rn →R is piecewise linear in case there is a ﬁnite set of polyhedra whose union is Rn, and f is aﬃne linear over each such polyhedron. In order to analyze ReLUn(k), we use another function class deﬁned as follows. We call a function g a p-term max function if it can be expressed as maximum of p aﬃne terms, that is, g(x) = max{ℓ1(x), . . . , ℓp(x)} where ℓi : Rn →R is aﬃne linear for i ∈[p]. Note that this also includes max functions with less than p terms, as some functions ℓi may coincide. Based on that, we deﬁne MAXn(p) := {f : Rn →R | f is a linear combination of p-term max functions}. Note that Wang and Sun [74] call p-term max functions (p−1)-order hinges and linear combinations of those (p −1)-order hinging hyperplanes. If the input dimension n is not important for the context, we sometimes drop the index and use ReLU(k) := S n∈N ReLUn(k) and MAX(p) := S n∈N MAXn(p) instead. We will use the standard notations conv A and cone A for the convex and conic hulls of a set A ⊆Rn. For an in-depth treatment of polyhedra and (mixed-integer) optimization, we refer to the book by Schrijver [64]. 1.2 Representing Piecewise Linear Functions with ReLU Networks It is not hard to see that every function expressed by a ReLU network is continuous and piecewise linear (CPWL) because it is composed of aﬃne transformations and ReLU functions, which are both CPWL. Based on a result by Wang and Sun [74], Arora et al. [6] prove that the converse is true as well by showing that any CPWL function can be represented with logarithmic depth. 3 x1 x2 x3 x4 y 1 1 -1 1 1 -1 1 1 -1 1 -1 1 -1 1 -1 1 -1 1 -1 1 -1 Figure 2: An NN to compute the maximum of four numbers that consists of three copies of the NN in Figure 1. Note that no activiation function is applied at the two unlabeled middle vertices (representing max{x1, x2} and max{x3, x4}). Therefore, the linear transformations directly before and after these vertices can be combined into a single one. Thus, the network has total depth three (two hidden layers). Theorem 1.1 (Arora et al. [6]). If n ∈N and k∗:= ⌈log2(n + 1)⌉, then CPWLn = ReLUn(k∗). Since this result is the starting point for our paper, let us brieﬂy sketch its proof. For this purpose, we start with a simple special case of a CPWL function: the maximum of n numbers. Recall that one hidden layer suﬃces to compute the maximum of two numbers, see Figure 1. Now one can easily stack this operation: in order to compute the maximum of four numbers, we divide them into two pairs with two numbers each, compute the maximum of each pair and then the maximum of the two results. This idea results in the NN depicted in Figure 2, which has two hidden layers. Repeating this procedure, one can compute the maximum of eight numbers with three hidden layers, and, in general, the maximum of 2k numbers with k hidden layers. Phrasing this the other way around, we obtain that the maximum of n numbers can be computed with ⌈log2(n)⌉hidden layers. Since NNs can easily form aﬃne combinations, this implies the following lemma. Lemma 1.2 (Arora et al. [6]). If n, k ∈N, then MAXn(2k) ⊆ReLUn(k). The question whether the depth of this construction is best possible is one of the central open questions we attack in this paper. In fact, the maximum function is not just a nice toy example, it is, in some sense, the most diﬃcult one of all CPWL function to represent for a ReLU NN. This is due to a result by Wang and Sun [74] stating that every CPWL function deﬁned on Rn can be written as linear combination of (n + 1)-term max functions. Theorem 1.3 (Wang and Sun [74]). If n ∈N, then CPWLn = MAXn(n + 1). The proof given by Wang and Sun [74] is technically involved and we do not go into details here. However, in Section 4 we provide an alternative proof yielding a slightly stronger result. This will be useful to bound the width of NNs representing arbitrary CPWL functions. 4 Theorem 1.1 by Arora et al. [6] can now be deduced from combining Lemma 1.2 and Theo- rem 1.3: In fact, for k∗= ⌈log2(n + 1)⌉, one obtains CPWLn = MAXn(n + 1) ⊆ReLUn(k∗) ⊆CPWLn and thus equality in the whole chain of subset relations. 1.3 Our Main Conjecture We wish to understand whether the logarithmic depth bound in Theorem 1.1 by Arora et al. [6] is best possible or whether one can do better. We believe it is indeed best possible and pose the following conjecture to better understand the importance of depth in neural networks. Conjecture 1.4. For every n ∈N, let k∗:= ⌈log2(n + 1)⌉. Then it holds that ReLUn(0) ⊊ReLUn(1) ⊊· · · ⊊ReLUn(k∗−1) ⊊ReLUn(k∗) = CPWLn . (1) Conjecture 1.4 claims that any additional layer up to k∗hidden layers strictly increases the set of representable functions. This would imply that the construction by Arora et al. [6] is actually depth-minimal. Observe that, in order to prove Conjecture 1.4, it is suﬃcient to ﬁnd, for every k∗∈N, one function f ∈ReLUn(k∗) \\ ReLUn(k∗−1) with n = 2k∗−1. This also implies all other strict inclu- sions ReLUn(i −1) ⊊ReLUn(i) for i < k∗because ReLUn(i −1) = ReLUn(i) immediately implies that ReLUn(i −1) = ReLUn(i′) for all i′ ≥i −1. In fact, thanks to Theorem 1.3 by Wang and Sun [74], there is a canonical candidate for such a function, allowing us to reformulate the conjecture as follows. Conjecture 1.5. For k ∈N, n = 2k, the function fn(x) = max{0, x1, . . . , xn} cannot be represented with k hidden layers, that is, fn /∈ReLUn(k). Proposition 1.6. Conjecture 1.4 and Conjecture 1.5 are equivalent. Proof. We argued above that Conjecture 1.5 implies Conjecture 1.4. For the other direction, we prove the contraposition, that is, assuming that Conjecture 1.5 is violated, we show that Con- jecture 1.4 is violated as well. To this end, suppose there is a k ∈N, n = 2k, such that fn is representable with k hidden layers. We argue that under this hypothesis, any (n + 1)-term max function can be represented with k hidden layers. To see this, observe that max{ℓ1(x), . . . , ℓn+1(x)} = max{0, ℓ1(x) −ℓn+1(x), . . . , ℓn(x) −ℓn+1(x)} + ℓn+1(x). Modifying the ﬁrst-layer weights of the NN computing fn such that input xi is replaced by the aﬃne expression ℓi(x) −ℓn+1(x), one obtains a k-hidden-layer NN computing the function max{0, ℓ1(x) −ℓn+1(x), . . . , ℓn(x) −ℓn+1(x)}. Moreover, since aﬃne functions, in particular also ℓn+1(x), can easily be represented by k-hidden-layer NNs, we obtain that any (n+1)-term maximum is in ReLUn(k). Using Theorem 1.3 by Wang and Sun [74], it follows that ReLUn(k) = CPWLn. In particular, since k∗:= ⌈log2(n + 1)⌉= k + 1, we obtain that Conjecture 1.4 must be violated as well. 5 x1 0 x2 x1 x2 · · · y Figure 3: Set of breakpoints of the function max{0, x1, x2} (left). This function cannot be computed by a 2-layer NN (middle), since the set of breakpoints of any function computed by such an NN is always a union of lines (right). It is known that Conjecture 1.5 holds for k = 1 [56], that is, the CPWL function max{0, x1, x2} cannot be computed by a 2-layer NN. The reason for this is that the set of breakpoints of a CPWL function computed by a 2-layer NN is always a union of lines, while the set of breakpoints of max{0, x1, x2} is a union of three half-lines; compare Figure 3 and the detailed proof by Mukherjee and Basu [56]. Moreover, in subsequent work to the ﬁrst version of this article, it was shown that the conjecture is true for all k ∈N if one only allows integer weights in the neural network [31]. However, this proof does not easily generalize to arbitrary, real-valued weights. Thus, the conjecture remains open for all k ≥2. 1.4 Contribution and Outline In this paper, we present the following results as partial progress towards resolving this conjecture. In Section 2, we resolve Conjecture 1.5 for k = 2, under a natural assumption on the breakpoints of the function represented by any intermediate neuron. Intuitively, the assumption states that no neuron introduces unexpected breakpoints compared to the ﬁnal function we want to represent. We call such neural networks H-conforming, see Section 2 for a formal deﬁnition. We then provide a computer-based proof leveraging techniques from mixed-integer programming for the following theorem. Theorem 1.7. There does not exist an H-conforming 3-layer ReLU NN computing the function max{0, x1, x2, x3, x4}. In the light of Lemma 1.2, stating that MAX(2k) ⊆ReLU(k) for all k ∈N, one might ask whether the converse is true as well, that is, whether the classes MAX(2k) and ReLU(k) are actually equal. This would not only provide a neat characterization of ReLU(k), but also prove Conjecture 1.5 without any additional assumption since one can show that max{0, x1, . . . , x2k} is not contained in MAX(2k). In fact, for k = 1, it is true that ReLU(1) = MAX(2), that is, a function is computable with one hidden layer if and only if it is a linear combination of 2-term max functions. However, in Section 3, we show the following theorem. Theorem 1.8. For every k ≥2, the set ReLU(k) is a strict superset of MAX(2k). To achieve this result, the key technical ingredient is the theory of polyhedral complexes asso- ciated with CPWL functions. This way, we provide important insights concerning the richness of the class ReLU(k). As a by-product, the results in Section 3 imply that MAXn(n) is a strict subset 6 of CPWLn = MAXn(n + 1), which was conjectured by Wang and Sun [74] in 2005, but has been open since then. So far, we have focused on understanding the smallest depth needed to express CPWL functions using neural networks with ReLU activations. In Section 4, we complement these results by upper bounds on the sizes of the networks needed for expressing arbitrary CPWL functions. In particular, we show the following theorem. Theorem 1.9. Let f : Rn →R be a CPWL function with p aﬃne pieces. Then f can be represented by a ReLU NN with depth ⌈log2(n + 1)⌉+ 1 and width O(p2n2+3n+1). We arrive at this result by introducing a novel application of recently established interrelations between neural networks and tropical geometry. Theorem 1.9 improves upon a previous bound by He et al. [35] because it is polynomial in p if n is regarded as ﬁxed constant, while the bounds in [35] are exponential in p. In subsequent work to the ﬁrst version of our article, it was shown that the width of the network can be drastically decreased if one allows more depth (in the order of log(p) instead of log(n)) [16]. Let us remark that there are diﬀerent deﬁnitions of the number of pieces p of a CPWL function f in the literature, compare the discussions in [16,35] about pieces versus linear components. Our bounds work with any of these deﬁnitions since they apply to the smallest possible way to deﬁne p, called linear components in [16]: for our purposes, p can be deﬁned as the smallest number of aﬃne functions such that, at each point, f is equal to one of these aﬃne functions. Since all other deﬁnitions of the number of pieces are at least that large, our bounds are valid for these deﬁnitions as well. Finally, in Section 5, we provide an outlook how these interactions between tropical geometry and NNs could possibly also be useful to provide a full, unconditional proof of Conjecture 1.4 by means of polytope theory. This yields another equivalent rephrasing of Conjecture 1.4 which is stated purely in the language of basic operations on polytopes and does not involve neural networks any more. We conclude in Section 6 with a discussion of further open research questions. 1.5 Further Related Work Depth versus size Soon after the original universal approximation theorems [18, 40], concrete bounds were obtained on the number of neurons needed in the hidden layer to achieve a certain level of accuracy. The literature on this is vast and we refer to a small representative sample here [8,9,51– 53,60]. More recent research has focused on how deeper networks can have exponentially or super exponentially smaller size compared to shallower networks [6,21,32,33,46,57,61,63,69,70,72,75]. See also [29] for another perspective on the relationship between expressivity and architecture, and the references therein. Mixed-integer optimization and machine learning Over the past decade, a growing body of work has emerged that explores the interplay between mixed-integer optimization and machine learning. On the one hand, researchers have attempted to improve mixed-integer optimization algorithms by exploiting novel techniques from machine learning [3,13,24,34,43–45,47]; see also [10] for a recent survey. On the ﬂip side, mixed-integer optimization techniques have been used to analyze function classes represented by neural networks [4,22,65–67]. In Section 2 below, we show 7 another new use of mixed-integer optimization tools for understanding function classes represented by neural networks. Design of training algorithms We believe that a better understanding of the function classes represented exactly by a neural architecture also has beneﬁts in terms of understanding the com- plexity of the training problem. For instance, in work by Arora et al. [6], an understanding of single layer ReLU networks enables the design of a globally optimal algorithm for solving the empirical risk minimization (ERM) problem, that runs in polynomial time in the number of data points in ﬁxed dimension. See also [1,11,12,14,17,19,23,25–28,42] for similar lines of work. Neural Networks and Tropical Geometry A recent stream of research involves the inter- play between neural networks and tropical geometry. The piecewise linear functions computed by neural networks can be seen as (tropical quotients of) tropical polynomials. Linear regions of these functions correspond to vertices of so-called Newton polytopes associated with these tropical polynomials. Applications of this correspondence include bounding the number of linear regions of a neural network [15,54,76] and understanding decision boundaries [2]. In Section 4 we present a novel application of tropical concepts to understand neural networks. We refer to [50] for a recent survey of connections between machine learning and tropical geometry, as well as to the textbooks by Maclagan and Sturmfels [49] and Joswig [41] for in-depth introductions to tropical geometry and tropical combinatorics. 2 Conditional Lower Depth Bounds via Mixed-Integer Program- ming In this section, we provide a computer-aided proof that, under a natural, yet unproven assumption, the function f(x) := max{0, x1, x2, x3, x4} cannot be represented by a 3-layer NN. It is worth to note that, to the best of our knowledge, no CPWL function is known for which the non-existence of a 3-layer NN can be proven without additional assumptions. For ease of notation, we write x0 := 0. We ﬁrst prove that we may restrict ourselves to NNs without biases. This holds true independent of our assumption, which we introduce afterwards. Deﬁnition 2.1. A function g: Rn →Rm is called positively homogeneous if it satisﬁes g(λx) = λg(x) for all λ ≥0. Deﬁnition 2.2. For an NN given by transformations T (ℓ)(x) = A(ℓ)x + b(ℓ), we deﬁne the corre- sponding homogenized NN to be the NN given by ˜T (ℓ)(x) = A(ℓ)x with all biases set to zero. Proposition 2.3. If an NN computes a positively homogeneous function, then the corresponding homogenized NN computes the same function. Proof. Let g: Rn0 →Rnk+1 be the function computed by the original NN and ˜g the one computed by the homogenized NN. Further, for any 0 ≤ℓ≤k, let g(ℓ) = T (ℓ+1) ◦σ ◦T (ℓ) ◦· · · ◦T (2) ◦σ ◦T (1) be the function computed by the sub-NN consisting of the ﬁrst (ℓ+ 1)-layers and let ˜g(ℓ) be the function computed by the corresponding homogenized sub-NN. We ﬁrst show by induction on ℓ 8 that the norm of ∥g(ℓ)(x) −˜g(ℓ)(x)∥is bounded by a global constant that only depends on the parameters of the NN but not on x. For ℓ= 0, we have ∥g(0)(x) −˜g(0)(x)∥= ∥b(1)∥=: C0, settling the induction base. For the induction step, let ℓ≥1 and assume that ∥g(ℓ−1)(x) −˜g(ℓ−1)(x)∥≤Cℓ−1, where Cℓ−1 only depends on the parameters of the NN. Since a component-wise application of the ReLU activation function has Lipschitz constant 1, this implies ∥(σ ◦g(ℓ−1))(x) −(σ ◦˜g(ℓ−1))(x)∥≤Cℓ−1. Using the spectral matrix norm ∥A∥of a matrix A, we obtain: ∥g(ℓ)(x) −˜g(ℓ)(x)∥= ∥b(ℓ+1) + A(ℓ+1)((σ ◦g(ℓ−1))(x) −(σ ◦˜g(ℓ−1))(x))∥ ≤∥b(ℓ+1)∥+ ∥A(ℓ+1)∥· Cℓ−1 =: Cℓ Since the right-hand side only depends on NN parameters, the induction is completed. Finally, we show that g = ˜g. For the sake of contradiction, suppose that there is an x ∈Rn0 with ∥g(x) −˜g(x)∥= δ > 0. Let x′ := Ck+1 δ x; then, by positive homogeneity of g (by assumption) and ˜g (by construction and because the ReLU function is positively homogeneous), it follows that ∥g(x′) −˜g(x′)∥= Ck + 1 > Ck, contradicting the property shown above. Thus, we have g = ˜g. Since f = max{0, x1, x2, x3, x4} is positively homogeneous, Proposition 2.3 implies that, if there is a 3-layer NN computing f, then there also is one that has no biases. Therefore, in the remainder of this section, we only consider NNs without biases and assume implicitly that all considered CPWL functions are positively homogeneous. In particular, any piece of such a CPWL function is linear and not only aﬃne linear. Observe that, for the function f, the only points of non-diﬀerentiability (a.k.a. breakpoints) are at places where at least two of the ﬁve numbers x0 = 0, x1, x2, x3, and x4 are equal. Hence, if some neuron of an NN computing f introduces breakpoints at other places, these breakpoints must be canceled out by other neurons. Therefore, we ﬁnd it natural to work under the assumption that such breakpoints need not be introduced at all in the ﬁrst place. To make this assumption formal, let Hij = {x ∈R4 | xi = xj}, for 0 ≤i < j ≤4, be ten hyperplanes in R4 and H = S 0≤i<j≤4 Hij be the corresponding hyperplane arrangement. This is the intersection of the so-called braid arrangement in ﬁve dimensions with the hyperplane x0 = 0 [68]. The regions or cells of H are deﬁned to be the closures of the connected components of R4 \\ H. It is easy to see that these regions are in one-to-one correspondence to the 5! = 120 possible orderings of the ﬁve numbers x0 = 0, x1, x2, x3, and x4. More precisely, for a permutation π of the ﬁve indices [4]0 = {0, 1, 2, 3, 4}, the corresponding region is the polyhedron Cπ := {x ∈R4 | xπ(0) ≤xπ(1) ≤xπ(2) ≤xπ(3) ≤xπ(4)}. Deﬁnition 2.4. We say that a (positively homogeneous) CPWL function g is H-conforming, if it is linear within any of these regions of H, that is, if it only has breakpoints where the relative ordering of the ﬁve values x0 = 0, x1, x2, x3, x4 changes. Moreover, an NN is said to be H-conforming if the output of each neuron contained in the NN is H-conforming. See Figure 4 for an illustration of the deﬁnition in the (simpler) two-dimensional case. Note that, by the deﬁnition, an NN is H-conforming if and only if, for all layers ℓ∈[k], the intermediate function σ ◦T (ℓ) ◦σ ◦T (ℓ−1) ◦· · · ◦σ ◦T (1) is H-conforming. As argued above, it is plausible that considering H-conforming NNs is enough to prove Conjec- ture 1.4. In other words, we conjecture that, if there exists a 3-layer NN computing the function 9 x1 ≥ x2 ≥0 0 ≥x2 ≥x1 x1 ≥0 ≥x2 x2 ≥ 0 ≥x1 x2 ≥x1 ≥0 0 ≥ x1 ≥x2 Figure 4: A function is H-conforming if the set of breakpoints is a subset of the hyperplane arrangement H. The arrangement H consists of all hyperplanes where two of the coordinates (possibly including x0 = 0) are equal. Here, H is illustrated for the (simpler) two-dimensional case, where it consists of three hyperplanes that divide the space into six cells. f(x) = max{0, x1, x2, x3, x4}, then there also exists one that is H-conforming. This motivates the following theorem, which we prove computer-aided by means of mixed-integer programming. Theorem 1.7. There does not exist an H-conforming 3-layer ReLU NN computing the function max{0, x1, x2, x3, x4}. The remainder of this section is devoted to proving this theorem. The rough outline of the proof is as follows. We ﬁrst study some geometric properties of the hyperplane arrangement H. This will show that each of the 120 cells of H is a simplicial polyhedral cone spanned by 4 extreme rays. In total, there are 30 such rays (because rays are used multiple times to span diﬀerent cones). This implies that each H-conforming function is uniquely determined by its values on the 30 rays and, therefore, the set of H-conforming functions of type R4 →R is a 30-dimensional vector space. We then use linear algebra to show that the space of functions generated by H-conforming two-layer NNs is a 14-dimensional subspace. Moreover, with two hidden layers, at least 29 of the 30 dimensions can be generated and f is not contained in this 29-dimensional subspace. So the remaining question is whether the 14 dimensions producible with the ﬁrst hidden layer can be combined in such a way that after applying a ReLU activation in the second hidden layer, we do not end up within the 29-dimensional subspace. We model this question as a mixed-integer program (MIP). Solving the MIP yields that we always end up within the 29-dimensional subspace, implying that f cannot be represented by a 3-layer NN. This provides a computational proof of Theorem 1.7. Let us start with investigating the structure of the hyperplane arrangement H. For readers familiar with the interplay between hyperplane arrangements and polytopes, it is worth noting that H is dual to a combinatorial equivalent of the 4-dimensional permutahedron. Hence, what we are studying in the following are some combinatorial properties of the permutahedron. Recall that the regions of H are given by the 120 polyhedra Cπ := {x ∈R4 | xπ(0) ≤xπ(1) ≤xπ(2) ≤xπ(3) ≤xπ(4)} ⊆R4 for each permutation π of [4]0, where x0 is used as a replacement for 0. With this representation, one can see that Cπ is a pointed polyhedral cone (with the origin as its only vertex) spanned by the four half-lines (a.k.a. rays) 10 R{π(0)} := {x ∈R4 | xπ(0) ≤xπ(1) = xπ(2) = xπ(3) = xπ(4)}, R{π(0),π(1)} := {x ∈R4 | xπ(0) = xπ(1) ≤xπ(2) = xπ(3) = xπ(4)}, R{π(0),π(1),π(2)} := {x ∈R4 | xπ(0) = xπ(1) = xπ(2) ≤xπ(3) = xπ(4)}, R{π(0),π(1),π(2),π(3)} := {x ∈R4 | xπ(0) = xπ(1) = xπ(2) = xπ(3) ≤xπ(4)}. Observe that these objects are indeed rays anchored at the origin because the three equalities deﬁne a one-dimensional subspace of R4 and the inequality cuts away one of the two directions. With that notation, we see that each of the 120 cells of H is a simplicial cone spanned by four out of the 30 rays RS with ∅⊊S ⊊[4]0. For each such set S, denote its complement by ¯S := [4]0\\S. Let us use a generating vector rS ∈R4 for each of these rays such that RS = cone rS as follows: If 0 ∈S, then rS := 1 ¯S ∈R4, otherwise rS := −1S ∈R4, where for each S ⊆[4], the vector 1S ∈R4 contains entries 1 at precisely those index positions that are contained in S and entries 0 elsewhere. For example, r{0,2,3} = (1, 0, 0, 1) ∈R4 and r{1,4} = (−1, 0, 0, −1) ∈R4. Then, the set R containing conic generators of all the 30 rays of H consists of the 30 vectors R = ({0, 1}4 ∪{0, −1}4) \\ {0}4. Let S30 be the space of all H-conforming CPWL functions of type R4 →R. We show that S30 is a 30-dimensional vector space. Lemma 2.5. The map g 7→(g(r))r∈R that evaluates a function g ∈S30 at the 30 rays in R is an isomorphism between S30 and R30. In particular, S30 is a 30-dimensional vector space. Proof. First note that S30 is closed under addition and scalar multiplication. Therefore, it is a subspace of the vector space of continuous functions of type R4 →R, and thus, in particular, a vector space. We show that the map g 7→(g(r))r∈R is in fact a vector space isomorphism. The map is obviously linear, so we only need to show that it is a bijection. In order to do so, remember that R4 is the union of the 5! = 120 simplicial cones Cπ. In particular, given the function values on the extreme rays of these cones, there is a unique positively homogeneous, continuous continuation that is linear within each of the 120 cones. This implies that the considered map is a bijection between S30 and R30. The previous lemma also provides a canonical basis of the vector space S30: the one consisting of all CPWL functions attaining value 1 at one ray r ∈R and value 0 at all other rays. However, it turns out that for our purposes it is more convenient to work with a diﬀerent basis. To this end, let gM(x) = maxi∈M xi for each M ⊆[4]0 with M /∈{∅, {0}}. These 30 functions contain, among other functions, the four (linear) coordinate projections g{i}(x) = xi, i ∈[4], and the function f(x) = g[4]0(x) = max{0, x1, x2, x3, x4}. Lemma 2.6. The 30 functions gM(x) = maxi∈M xi with {∅, {0}} ̸∋M ⊆[4]0 form a basis of S30. Proof. Evaluating the 30 functions gM at all 30 rays r ∈R yields 30 vectors in R30. It can be easily veriﬁed (e.g., using a computer) that these vectors form a basis of R30. Thus, due to the isomorphism of Lemma 2.5, the functions gM form a basis of S30. Next, we focus on particular subspaces of S30 generated by only some of the 30 functions gM. We prove that they correspond to the spaces of functions computable by H-conforming 2- and 3-layer NNs, respectively. 11 To do so, let B14 be the set of the 14 basis functions gM with {∅, {0}} ̸∋M ⊆[4]0 and |M| ≤2. Let S14 be the 14-dimensional subspace spanned by B14. Similarly, let B29 be the set of the 29 basis functions gM with {∅, {0}} ̸∋M ⊊[4]0 (all but [4]0). Let S29 be the 29-dimensional subspace spanned by B29. Lemma 2.7. The space S14 consists of all functions computable by H-conforming 2-layer NNs. Proof. Each function in S14 is a linear combination of 2-term max functions by deﬁnition. Hence, by Lemma 1.2, it can be represented by a 2-layer NN. Conversely, we show that any function representable by a 2-layer NN is indeed contained in S14. It suﬃces to show that the output of every neuron in the ﬁrst (and only) hidden layer of an H- conforming ReLU NN is in S14 because the output of a 2-layer NN is a linear combination of such outputs. Let a ∈R4 be the ﬁrst-layer weights of such a neuron, computing the function ga(x) := max{aT x, 0}, which has the hyperplane {x ∈R4 | aT x = 0} as breakpoints (or is constantly zero). Since the NN must be H-conforming, this must be one of the ten hyperplanes xi = xj, 0 ≤i < j ≤4. Thus, ga(x) = max{λ(xi −xj), 0} for some λ ∈R. If λ ≥0, it follows that ga = λg{i,j} −λg{j} ∈S14, and if λ ≤0, we obtain ga = −λg{i,j} + λg{i} ∈S14. This concludes the proof. For 3-layer NNs, an analogous statement can be made. However, only one direction can be easily seen. Lemma 2.8. Any function in S29 can be represented by an H-conforming 3-layer NN. Proof. As in the previous lemma, each function in S29 is a linear combination of 4-term max functions by deﬁnition. Hence, by Lemma 1.2, it can be represented by a 3-layer NN. Our goal is to prove the converse as well: any H-conforming function represented by a 3-layer NN is in S29. Since f(x) = max{0, x1, x2, x3, x4} is the 30th basis function, which is linearly independent from B29 and thus not contained in S29, this implies Theorem 1.7. To achieve this goal, we ﬁrst provide another characterization of S29, which can be seen as an orthogonal direction to S29 in S30. For a function g ∈S30, let φ(g) := X ∅⊊S⊊[4]0 (−1)|S|g(rS) be a linear map from S30 to R. Lemma 2.9. A function g ∈S30 is contained in S29 if and only if φ(g) = 0. Proof. Any g ∈S30 can be represented as a unique linear combination of the 30 basis functions gM and is contained in S29 if and only if the coeﬃcient of f = g[4]0 is zero. One can easily check (with a computer) that φ maps all functions in B29 to 0, but not the 30th basis function f. Thus, g is contained in S29 if and only if it satisﬁes φ(g) = 0. In order to make use of our assumption that the NN is H-conforming, we need the follow- ing insight about when the property of being H-conforming is preserved after applying a ReLU activation. 12 Lemma 2.10. Let g ∈S30. The function h = σ ◦g is H-conforming (and thus in S30 as well) if and only if there is no pair of sets ∅⊊S ⊊S′ ⊊[4]0 with g(rS) and g(rS′) being nonzero and having diﬀerent signs. Proof. The key observation to prove this lemma is the following: for two rays rS and rS′, there exists a cell C of the hyperplane arrangement H for which both rS and rS′ are extreme rays if and only if S ⊊S′ or S′ ⊊S. Hence, if there exists a pair of sets ∅⊊S ⊊S′ ⊊[4]0 with g(rS) and g(rS′) being nonzero and having diﬀerent signs, then the function g restricted to C is a linear function with both strictly positive and strictly negative values. Therefore, after applying the ReLU activation, the resulting function h has breakpoints within C and is not H-conforming. Conversely, if for each pair of sets ∅⊊S ⊊S′ ⊊[4]0, both g(rS) and g(rS′) are either nonpositive or nonnegative, then g restricted to any cell C of H is either nonpositive or nonnegative everywhere. In the ﬁrst case, h restricted to that cell C is the zero function, while in the second case, h coincides with g in C. In both cases, h is linear within all cells and, thus, H-conforming. Having collected all these lemmas, we are ﬁnally able to construct an MIP whose solution proves that any function computed by an H-conforming 3-layer NN is in S29. As in the proof of Lemma 2.7, it suﬃces to focus on the output of a single neuron in the second hidden layer. Let h = σ ◦g be the output of such a neuron with g being its input. Observe that, by construction, g is a function computed by a 2-layer NN, and thus, by Lemma 2.7, a linear combination of the 14 functions in B14. The MIP contains three types of variables, which we denote in bold to distinguish them from constants: • 14 continuous variables aM ∈[−1, 1], being the coeﬃcients of the linear combination of the basis of S14 forming g, that is, g = P gM∈B14 aMgM (since multiplying g and h with a nonzero scalar does not alter the containment of h in S29, we may restrict the variables to [−1, 1]), • 30 binary variables zS ∈{0, 1} for ∅⊊S ⊊[4]0, determining whether the considered neuron is strictly active at ray rS, that is, whether g(rS) > 0, • 30 continuous variables yS ∈R for ∅⊊S ⊊[4]0, representing the output of the considered neuron at all rays, that is, yS = h(rS). To ensure that these variables interact as expected, we need two types of constraints: • For each of the 30 rays rS, ∅⊊S ⊊[4]0, the following constraints ensure that zS and output yS are correctly calculated from the variables aM, that is, zS = 1 if and only if g(rS) = P gM∈B14 aMgM(rS) is positive, and yS = max{0, g(rS)}. Also compare the references given in Section 1.5 concerning MIP models for ReLU units. Note that the restriction of the coeﬃcients aM to [−1, 1] ensures that the absolute value of g(rS) is always bounded by 14, allowing us to use 15 as a replacement for +∞: yS ≥0 yS ≥ X gM∈B14 aMgM(rS) yS ≤15zS yS ≤ X gM∈B14 aMgM(rS) + 15(1 −zS) (2) 13 Observe that these constraints ensure that one of the following two cases occurs: If zS = 0, then the ﬁrst and third line imply yS = 0 and the second line implies that the incoming activation is in fact nonpositive. The fourth line is always satisﬁed in that case. Otherwise, if zS = 1, then the second and fourth line imply that yS equals the incoming activation, and, in combination with the ﬁrst line, this has to be nonnegative. The third line is always satisﬁed in that case. Hence, the set of constraints (2) correctly models the ReLU activation function. • For each of the 150 pairs of sets ∅⊊S ⊊S′ ⊊[4]0, the following constraints ensure that the property in Lemma 2.10 is satisﬁed. More precisely, if one of the variables zS or zS′ equals 1, then the ray of the other set has nonnegative activation, that is, g(rS′) ≥0 or g(rS) ≥0, respectively: X gM∈B14 aMgM(rS) ≥15(zS′ −1) X gM∈B14 aMgM(rS′) ≥15(zS −1) (3) Observe that these constraints successfully prevent that the two rays rS and rS′ have nonzero activations with diﬀerent signs. Conversely, if this is not the case, then we can always satisfy constraints (3) by setting only those variables zS to value 1 where the activation of ray rS is strictly positive. (Note that, if the incoming activation is precisely zero, constraints (2) make it possible to choose both values 0 or 1 for zS.) Hence, these constraints are in fact appropriate to model H-conformity. In the light of Lemma 2.9, the objective function of our MIP is to maximize φ(h), that is, the expression X ∅⊊S⊊[4]0 (−1)|S|yS. The MIP has a total of 30 binary and 44 continuous variables, as well as 420 inequality con- straints. The next proposition formalizes how this MIP can be used to check whether a 3-layer NN function can exist outside S29. Proposition 2.11. There exists an H-conforming 3-layer NN computing a function not contained in S29 if and only if the objective value of the MIP deﬁned above is strictly positive. Proof. For the ﬁrst direction, assume that such an NN exists. Since its ﬁnal output is a linear combination of the outputs of the neurons in the second hidden layer, one of these neurons must compute a function ˜h = σ ◦˜g /∈S29, with ˜g being the input to that neuron. By Lemma 2.9, it follows that φ(˜h) ̸= 0. Moreover, we can even assume without loss of generality that φ(˜h) > 0, as we argue now. If this is not the case, multiply all ﬁrst-layer weights of the NN by −1 to obtain a new NN computing function ˆh instead of ˜h. Observing that rS = −r[4]0\\S for all rS ∈R, we obtain ˆh(rS) = ˜h(−rS) = ˜h(r[4]0\\S) for all rS ∈R. Plugging this into the deﬁnition of φ and using that the cardinalities of S and [4]0 \\ S have diﬀerent parity, we further obtain φ(ˆh) = −φ(˜h). Therefore, we can assume that φ(˜h) was already positive in the ﬁrst place. Using Lemma 2.7, the function ˜g can be represented as a linear combination ˜g = P gM∈B14 ˜aMgM of the functions in B14. Let α := maxM|˜aM|. Note that α > 0 because otherwise ˜g would 14 be the zero function. Let us deﬁne modiﬁed functions g and h from ˜g and ˜h as follows. Let aM := ˜aM/α ∈[−1, 1], g := P gM∈B14 aMgM, and h := σ ◦g. Moreover, for all rays rS ∈R, let yS := h(rS), as well as zS := 1 if yS > 0, and zS := 0 otherwise. It is easy to verify that the variables aM, yS, and zS deﬁned that way satisfy (2). Moreover, since the NN is H-conforming, they also satisfy (3). Finally, they also yield a strictly positive objective function value since φ(h) = φ(˜h)/α > 0. For the reverse direction, assume that there exists an MIP solution consisting of aM, yS, and zS, satisfying (2) and (3), and having a strictly positive objective function value. Deﬁne the functions g := P gM∈B14 aMgM and h := σ ◦g. One concludes from (2) that h(rS) = yS for all rays rS ∈R. Lemma 2.7 implies that g can be represented by a 2-layer NN. Thus, h can be represented by a 3-layer NN. Moreover, constraints (3) guarantee that this NN is H-conforming. Finally, since the MIP solution has strictly positive objective function value, we obtain φ(h) > 0, implying that h /∈S29. In order to use the MIP as part of a mathematical proof, we employed an MIP solver that uses exact rational arithmetics without numerical errors, namely the solver by the Parma Polyhedral Library (PPL) [7]. We called the solver from a SageMath (Version 9.0) [71] script on a machine with an Intel Core i7-8700 6-Core 64-bit CPU and 15.5 GB RAM, using the openSUSE Leap 15.2 Linux distribution. SageMath, which natively includes the PPL solver, is published under the GPLv3 license. After a total running time of almost 7 days (153 hours), we obtained optimal objective function value zero. This makes it possible to prove Theorem 1.7. Proof of Theorem 1.7. Since the MIP has optimal objective function value zero, Proposition 2.11 implies that any function computed by an H-conforming 3-layer NN is contained in S29. In partic- ular, it is not possible to compute the function f(x) = max{0, x1, x2, x3, x4} with an H-conforming 3-layer NN. We remark that state-of-the-art MIP solver Gurobi (version 9.1.1) [30], which is commercial but oﬀers free academic licenses, is able to solve the same MIP within less than a second, providing the same result. However, Gurobi does not employ exact arithmetics, making it impossible to exclude numerical errors and use it as a mathematical proof. The SageMath code can be found on GitHub at https://github.com/ChristophHertrich/relu-mip-depth-bound. Additionally, the MIP can be found there as .mps ﬁle, a standard format to represent MIPs. This allows one to use any solver of choice to reproduce our result. 3 Going Beyond Linear Combinations of Max Functions In this section we prove the following result, showing that NNs with k hidden layers can compute more functions than only linear combinations of 2k-term max functions. Theorem 1.8. For every k ≥2, the set ReLU(k) is a strict superset of MAX(2k). In order to prove this theorem, for each number of hidden layers k ≥2, we provide a speciﬁc function in ReLU(k) \\ MAX(2k). The challenging part is to show that the function is in fact not contained in MAX(2k). 15 Proposition 3.1. For any n ≥3, the function f : Rn →R deﬁned by f(x) = max{0, x1, x2, . . . , xn−3, max{xn−2, xn−1} + max{0, xn}} (4) is not contained in MAX(n). This means that f cannot be written as a linear combination of n-term max functions, which proves a conjecture by [74] that MAXn(n) ⊊CPWLn, which has been open since 2005. Previously, it was only known that linear combinations of (n −1)-term maxes are not suﬃcient to represent any CPWL function deﬁned on Rn, that is, MAXn(n −1) ⊊CPWLn. Lu [48] provides a short analytical argument for this fact. Before we prove Proposition 3.1, we show that it implies Theorem 1.8. Proof of Theorem 1.8. For k ≥2, let n := 2k. By Proposition 3.1, function f deﬁned in (4) is not contained in MAX(2k). It remains to show that it can be represented using a ReLU NN with k hidden layers. To see this, ﬁrst observe that any of the n/2 = 2k−1 terms max{0, x1}, max{x2i, x2i+1} for i ∈[n/2 −2], and max{xn−2, xn−1} + max{0, xn} can be expressed by a one- hidden-layer NN since all these are (linear combinations of) 2-term max functions. Since f is the maximum of these 2k−1 terms, and since the maximum of 2k−1 numbers can be computed with k −1 hidden layers (Lemma 1.2), this implies that f is in ReLU(k). In order to prove Proposition 3.1, we need the concept of polyhedral complexes. A polyhedral complex P is a ﬁnite set of polyhedra such that each face of a polyhedron in P is also in P, and for two polyhedra P, Q ∈P, their intersection P ∩Q is a common face of P and Q (possibly the empty face). Given a polyhedral complex P in Rn and an integer m ∈[n], we let Pm denote the collection of all m-dimensional polyhedra in P. For a convex CPWL function f, we deﬁne its underlying polyhedral complex as follows: it is the unique polyhedral complex covering Rn (i.e., each point in Rn belongs to some polyhedron in P) whose n-dimensional polyhedra coincide with the domains of the (maximal) aﬃne pieces of f. In particular, f is aﬃne linear within each P ∈P, but not within any strict superset of a polyhedron in Pn. Exploiting properties of polyhedral complexes associated with CPWL functions, we prove the following proposition below. Proposition 3.2. Let f0 : Rn →R be a convex CPWL function and let P0 be the underlying polyhedral complex. If there exists a hyperplane H ⊆Rn such that the set T := [ F ∈Pn−1 0 \f\f F ⊆H \t is nonempty and contains no line, then f0 cannot be expressed as a linear combination of n-term maxima of aﬃne linear functions. Again, before we proceed to the proof of Proposition 3.2, we show that it implies Proposition 3.1. Proof of Proposition 3.1. Observe that f (deﬁned in (4)) has the alternate representation f(x) = max{0, x1, x2, . . . , xn−3, xn−2, xn−1, xn−2 + xn, xn−1 + xn} as a maximum of n + 2 terms. Let P be its underlying polyhedral complex. Let the hyperplane H be deﬁned by x1 = 0. 16 Observe that any facet in Pn−1 is a polyhedron deﬁned by two of the n+2 terms that are equal and at least as large as each of the remaining n terms. Hence, the only facet that could possibly be contained in H is F := {x ∈Rn | x1 = 0 ≥x2, . . . , xn−3, xn−2, xn−1, xn−2 + xn, xn−1 + xn}. Note that F is indeed an (n −1)-dimensional facet in Pn−1, because, for example, a small ball around (0, −1, . . . , −1) ∈Rn intersected with H is contained in F. Finally, we need to show that F is pointed, that is, it contains no line. A well-known fact from polyhedral theory says if there is any line in F with direction d ∈Rn \\ {0}, then d must satisfy the deﬁning inequalities with equality. However, only the zero vector does this. Hence, F cannot contain a line. Therefore, when applying Proposition 3.2 to f with underlying polyhedral complex P and hyperplane H, we have T = F, which is nonempty and contains no line. Hence, f cannot be written as linear combination of n-term maxima. The remainder of this section is devoted to proving Proposition 3.2. In order to exploit properties of the underlying polyhedral complex of the considered CPWL functions, we will ﬁrst introduce some terminology, notation, and results related to polyhedral complexes in Rn for any n ≥1. Deﬁnition 3.3. Given an abelian group (G, +), we deﬁne Fn(G) as the family of all functions φ of the form φ: Pn →G, where P is a polyhedral complex that covers Rn. We say that P is the underlying polyhedral complex, or the polyhedral complex associated with φ. Just to give an intuition of the reason for this deﬁnition, let us mention that later we will choose (G, +) to be the set of aﬃne linear maps Rn →R with respect to the standard operation of sum of functions. Moreover, given a convex CPWL function f : Rn →R with underlying polyhedral complex P, we will consider the following function φ ∈Fn(G): for every P ∈Pn, φ(P) will be the aﬃne linear map that coincides with f over P. It can be helpful, though not necessary, to keep this in mind when reading the next deﬁnitions and observations. It is useful to observe that the functions in Fn(G) can also be described in a diﬀerent way. Before explaining this, we need to deﬁne an ordering between the two elements of each pair of opposite halfspaces. More precisely, let H be a hyperplane in Rn and let H′, H′′ be the two closed halfspaces delimited by H. We choose an arbitrary rule to say that H′ “precedes” H′′, which we write as H′ ≺H′′.1 We can then extend this ordering rule to those pairs of n-dimensional polyhedra of a polyhedral complex in Rn that share a facet. Speciﬁcally, given a polyhedral complex P in Rn, let P ′, P ′′ ∈Pn be such that F := P ′ ∩P ′′ ∈Pn−1. Further, let H be the unique hyperplane containing F. We say that P ′ ≺P ′′ if the halfspace delimited by H and containing P ′ precedes the halfspace delimited by H and containing P ′′. We can now explain the alternate description of the functions in Fn(G), which is based on the following notion. Deﬁnition 3.4. Let φ ∈Fn(G), with associated polyhedral complex P. The facet-function associ- ated with φ is the function ψ: Pn−1 →G deﬁned as follows: given F ∈Pn−1, let P ′, P ′′ be the two polyhedra in Pn such that F = P ′ ∩P ′′, where P ′ ≺P ′′; then we set ψ(F) := φ(P ′) −φ(P ′′). 1In case one wants to see such a rule explicitly, this is a possible way: Fix an arbitrary ¯x ∈H. We can say that H′ ≺H′′ if and only if ¯x + ei ∈H′, where ei is the ﬁrst vector in the standard basis of Rd that does not lie on H (i.e., e1, . . . , ei−1 ∈H and ei /∈H). Note that this deﬁnition does not depend on the choice of ¯x. 17 Although it will not be used, we observe that knowing ψ is suﬃcient to reconstruct φ up to an additive constant. This means that a function φ′ ∈Fn(G) associated with the same polyhedral complex P has the same facet-function ψ if and only if there exists g ∈G such that φ(P)−φ′(P) = g for every P ∈Pn. (However, it is not true that every function ψ: Pn−1 →G is the facet-function of some function in Fn(G).) We now introduce a sum operation over Fn(G). Deﬁnition 3.5. For functions φ1, . . . , φp ∈Fn(G) with associated polyhedral complexes P1, . . . , Pp, the sum φ := φ1 + · · · + φp is the function in Fn(G) deﬁned as follows: • the polyhedral complex associated with φ is P := {P1 ∩· · · ∩Pp | Pi ∈Pi for every i}; • given P ∈Pn, P can be uniquely obtained as P1 ∩· · · ∩Pp, where Pi ∈Pn i for every i; we then deﬁne φ(P) = p X i=1 φi(Pi). The term “sum” is justiﬁed by the fact that when P1 = · · · = Pp (and thus φ1, . . . , φp have the same domain) we obtain the standard notion of the sum of functions. The next results shows how to compute the facet-function of a sum of functions in Fn(G). Observation 3.6. With the notation of Deﬁnition 3.5, let ψ1, . . . , ψp be the facet-functions asso- ciated with φ1, . . . , φp, and let ψ be the facet-function associated with φ. Given F ∈Pn−1, let I be the set of indices i ∈{1, . . . , p} such that Pn−1 i contains a (unique) element Fi with F ⊆Fi. Then ψ(F) = X i∈I ψi(Fi). (5) Proof. Let P ′, P ′′ be the two polyhedra in Pn such that F = P ′ ∩P ′′, with P ′ ≺P ′′. We have P ′ = P ′ 1 ∩· · · ∩P ′ p and P ′′ = P ′′ 1 ∩· · · ∩P ′′ p for a unique choice of P ′ i, P ′′ i ∈Pn i for every i. Then ψ(F) = φ(P ′) −φ(P ′′) = p X i=1 (φi(P ′ i) −φi(P ′′ i )). (6) Now ﬁx i ∈[p]. Since F ⊆P ′ i ∩P ′′ i , dim(P ′ i ∩P ′′ i ) ≥n −1. If dim(P ′ i ∩P ′′ i ) = n −1, then Fi := P ′ i ∩P ′′ i ∈Pn−1 i and φi(P ′ i) −φi(P ′′ i ) = ψi(Fi). Furthermore, i ∈I because F ⊆Fi. If, on the contrary, dim(P ′ i ∩P ′′ i ) = n, the fact that Pi is a polyhedral complex implies that P ′ i = P ′′ i , and thus φi(P ′ i) −φi(P ′′ i ) = 0. Moreover, in this case i /∈I: this is because P ′ ∪P ′′ ⊆P ′ i, which implies that the relative interior of F is contained in the relative interior of P ′ i. With these observations, from (6) we obtain (5). Deﬁnition 3.7. Fix φ ∈Fn(G), with associated polyhedral complex P. Let H be a hyperplane in Rn, and let H′, H′′ be the closed halfspaces delimited by H. Deﬁne the polyhedral complex bP = {P ∩H | P ∈P} ∪{P ∩H′ | P ∈P} ∪{P ∩H′′ | P ∈P}. The reﬁnement of φ with respect to H is the function bφ ∈Fn(G) with associated polyhedral complex bP deﬁned as follows: given bP ∈bPn, bφ( bP) := φ(P), where P is the unique polyhedron in P that contains bP. 18 The next results shows how to compute the facet-function of a reﬁnement. Observation 3.8. With the notation of Deﬁnition 3.7, let ψ be the facet-function associated with φ. Then, the facet-function bψ associated with bφ is given by bψ( bF) = ( ψ(F) if there exists a (unique) F ∈Pn−1 containing bF 0 otherwise, for every bF ∈bPn−1. Proof. Let bP ′, bP ′′ be the polyhedra in bPn such that bF = bP ′ ∩bP ′′, with bP ′ ≺bP ′′. Further, let P ′, P ′′ be the unique polyhedra in Pn that contain bP ′, bP ′′ (respectively). It might happen that P ′ = P ′′. If there is F ∈Pn−1 containing bF, then the fact that P is a polyhedral complex implies that F = P ′ ∩P ′′. Note that P ′ ̸= P ′′ and P ′ ≺P ′′ in this case. Thus bψ( bF) = bφ( bP ′) −bφ( bP ′′) = φ(P ′) −φ(P ′′) = ψ(F). Assume now that no element of Pn−1 contains bF. Then there exists P ∈Pn such that bF = P ∩H and H intersects the interior of P. Note that P = P ′ = P ′′ in this case. Then bP ′ = P ∩H′ and bP ′′ = P ∩H′′ (or vice versa). It follows that bψ( bF) = bφ( bP ′) −bφ( bP ′′) = φ(P) −φ(P) = 0. We now prove that the operations of sum and reﬁnement commute: the reﬁnement of a sum is the sum of the reﬁnements. Observation 3.9. Let φ1, . . . , φp ∈Fn(G) be p functions with associated polyhedral complexes P1, . . . , Pp. Deﬁne φ := φ1 + · · · + φp. Let H be a hyperplane in Rn, and let H′, H′′ be the closed halfspaces delimited by H. Then bφ = bφ1 + · · · + bφp. Proof. Deﬁne eφ := bφ1 + · · · + bφp. It can be veriﬁed that bφ and eφ are deﬁned on the same poyhedral complex, which we denote by bP. We now ﬁx bP ∈bPn and show that bφ( bP) = eφ( bP). Since bP ∈bPn, it is n-dimensional and either contained in H′ or H′′. Since both cases are symmetric, let us focus on bP ⊆H′. This means, we can write it as bP = P1 ∩· · · ∩Pp ∩H′, where Pi ∈Pn i for every i. Then bφ( bP) = φ(P1 ∩· · · ∩Pp) = p X i=1 φi(Pi) = p X i=1 bφi(Pi ∩H′) = eφ(P1 ∩· · · ∩Pp ∩H′) = eφ(P), where the ﬁrst and third equations follow from the deﬁnition of reﬁnement, while the second and fourth equations follow from the deﬁnition of the sum. The lineality space of a (nonempty) polyhedron P = {x ∈Rn | Ax ≤b} is the null space of the constraint matrix A. In other words, it is the set of vectors y ∈Rn such that for every x ∈P the whole line {x + λy | λ ∈R} is a subset of P. We say that the lineality space of P is trivial, if it contains only the zero vector, and nontrivial otherwise. Given a polyhedron P, it is well-known that all nonempty faces of P share the same lineality space. Therefore, given a polyhedral complex P that covers Rn, all the nonempty polyhedra in P share the same lineality space L. We will call L the lineality space of P. 19 Lemma 3.10. Given an abelian group (G, +), pick φ1, . . . , φp ∈Fn(G), with associated polyhedral complexes P1, . . . , Pp. Assume that for every i ∈[p] the lineality space of Pi is nontrivial. Deﬁne φ := φ1 + · · · + φp, P as the underlying polyhedral complex, and ψ as the facet-function of φ. Then for every hyperplane H ⊆Rn, the set S := [ F ∈Pn−1 | F ⊆H, ψ(F) ̸= 0 \t is either empty or contains a line. Proof. The proof is by induction on n. For n = 1, the assumptions imply that all Pi are equal to P, and each of these polyhedral complexes has R as its only nonempty face. Since Pn−1 is empty, no hyperplane H such that S ̸= ∅can exist. Now ﬁx n ≥2. Assume by contradiction that there exists a hyperplane H such that S is nonempty and contains no line. Let bφ be the reﬁnement of φ with respect to H, bP be the underlying polyhedral complex, and bψ be the associated facet-function. Further, we deﬁne Q := {P ∩H | P ∈ bP}, which is a polyhedral complex that covers H. Note that if H is identiﬁed with Rn−1 then we can think of Q as a polyhedral complex that covers Rn−1, and the restriction of bψ to Qn−1, which we denote by φ′, can be seen as a function in Fn−1(G). We will prove that φ′ does not satisfy the lemma, contradicting the inductive hypothesis. Since φ = φ1 + · · · + φp, by Observation 3.9 we have bφ = bφ1 + · · · + bφp. Note that for every i ∈[p] the hyperplane H is covered by the elements of bPn−1. This implies that for every bF ∈bPn−1 and i ∈[p] there exists bFi ∈bPn−1 i such that bF ⊆bFi. Then, by Observation 3.6, bψ( bF) = bψ1( bF1) + · · · + bψp( bFp). Now, additionally suppose that bF is contained in H, that is, bF ∈Qn−1. Let i ∈[p] be such that the lineality space of Pi is not a subset of the linear space parallel to H. Then no element of Pn−1 i contains bFi. By Observation 3.8, bψi( bFi) = 0. We then conclude that bψ( bF) = X i∈J bψi( bFi) for every bF ∈Qn−1, where J is the set of indices i such that the lineality space of Pi is a subset of the linear space parallel to H. This means that φ′ = X i∈J φ′ i, where φ′ i is the restriction of bψi to Qn−1 i , with Qi := {P ∩H | P ∈c Pi}. Note that for every i ∈J the lineality space of Qi is clearly nontrivial, as it coincides with the lineality space of Pi. Now pick any bF ∈Qn−1. Note that if there exists F ∈Pn−1 such that bF ⊆F, then bF = F. It then follows from Observation 3.8 that [ n bF ∈Qn−1 \f\f\f bψ( bF) ̸= 0 o = S. In other words, [ F ∈Qn−1 \f\f φ′(F) ̸= 0 \t = S. (7) Since S ̸= H (as S contains no line), there exists a polyhedron F ∈Qn−1 such that F ⊆S and F has a facet F0 which does not belong to any other polyhedron in Qn−1 contained in S. Then the 20 facet-function ψ′ associated with φ′ satisﬁes ψ′(F0) ̸= 0. Let H′ be the (n −2)-dimensional aﬃne space containing F0. Then the set S′ := [ F ∈Qn−2 \f\f F ⊆H′, ψ′(F) ̸= 0 \t is nonempty, as F0 ⊆S′. Furthermore, we claim that S′ contains no line. To see why this is true, take any F ∈Qn−2 such that F ⊆H′ and ψ′(F) ̸= 0, and let F ′, F ′′ be the two polyhedra in Qn−1 having F as facet. Then φ′(F ′) ̸= φ′(F ′′), and thus at least one of these values (say φ′(F ′)) is nonzero. Then, by (7), F ′ ⊆S, and thus also F ⊆S. This shows that S′ ⊆S and therefore S′ contains no line. We have shown that φ′ does not satisfy the lemma. This contradicts the inductive assumption that the lemma holds in dimension n −1. Finally, we can use this lemma to prove Proposition 3.2. Proof of Proposition 3.2. Assume for the sake of a contradiction that f0(x) = p X i=1 λi max{ℓi1(x), . . . , ℓin(x)} for every x ∈Rn, where p ∈N, λ1, . . . , λp ∈R and ℓij : Rn →R is an aﬃne linear function for every i ∈[p] and j ∈[n]. Deﬁne fi(x) := λi max{ℓi1(x), . . . , ℓin(x)} for every i ∈[p], which is a CPWL function. Fix any i ∈[p] such that λi ≥0. Then fi is convex. Note that its epigraph Ei := {(x, z) ∈Rn × R | z ≥ℓij(x) for j ∈[n]} is a polyhedron in Rn+1 deﬁned by n inequalities, and thus has nontrivial lineality space. Fur- thermore, no line orthogonal to the x-space is contained in Ei. Since the underlying polyhedral complex Pi of fi consists of the orthogonal projections of the faces of Ei (excluding Ei itself) onto the x-space, this implies that Pi has also nontrivial lineality space. (More precisely, the lineality space of Pi is the projection of the lineality space of Ei.) If λi < 0, then fi is concave. By arguing as above on the convex function −fi, one obtains that the underlying polyhedral complex Pi has again nontrivial lineality space. Thus this property holds for every i ∈[p]. The set of aﬃne linear functions Rn →R forms an abelian group (with respect to the standard operation of sum of functions), which we denote by (G, +). For every i ∈[p]0, let φi be the function in Fn(G) with underlying polyhedral complex Pi deﬁned as follows: for every P ∈Pn i , φi(P) is the aﬃne linear function that coincides with fi over P. Deﬁne φ := φ1 + · · · + φp and let P be the underlying polyhedral complex. Note that for every P ∈Pn, φ(P) is precisely the aﬃne linear function that coincides with f0 within P. However, P may not coincide with P0, as there might exist P ′, P ′′ ∈Pd sharing a facet such that φ(P ′) = φ(P ′′); when this happens, f0 is aﬃne linear over P ′ ∪P ′′ and therefore P ′ and P ′′ are merged together in P0. Nonetheless, P is a reﬁnement of P0, i.e., for every P ∈Pn 0 there exist P1, . . . , Pk ∈Pn (for some k ≥1) such that P = P1 ∪· · · ∪Pk. Moreover, φ0(P) = φ(P1) = · · · = φ(Pk). Denoting by ψ the facet-function associated with φ, this implies for a facet F ∈Pn−1 that ψ(F) = 0 if and only if F is not subset of any facet F ′ ∈Pn−1 0 . 21 Let H be a hyperplane as in the statement of the proposition. The above discussion shows that T = [ F ∈Pn−1 0 \f\f F ⊆H \t = [ F ∈Pn−1 \f\f F ⊆H, ψ(F) ̸= 0 \t . Using S := T, we obtain a contradiction to Lemma 3.10. 4 A Width Bound for Neural Networks with Small Depth While the proof of Theorem 1.1 by Arora et al. [6] shows that CPWLn = ReLUn(⌈log2(n + 1)⌉), it does not provide any bound on the width of the NN required to represent any particular CPWL function. The purpose of this section is to prove that for ﬁxed dimension n, the required width for exact, depth-minimal representation of a CPWL function can be polynomially bounded in the number p of aﬃne pieces; speciﬁcally by pO(n2). This improves previous bounds by He et al. [35] and is closely related to works that bound the number of linear pieces of an NN as a function of the size [54,55,59,61]. It can also be seen as a counterpart, in the context of exact representations, to quantitative universal approximation theorems that bound the number of neurons required to achieve a certain approximation guarantee; see, e.g., [8,9,52,53,60]. 4.1 The Convex Case We ﬁrst derive our result for the case of convex CPWL functions and then use this to also prove the general nonconvex case. Our width bound is a consequence of the following theorem about convex CPWL functions, for which we are going to provide a geometric proof later. Theorem 4.1. Let f(x) = max{aT i x + bi | i ∈[p]} be a convex CPWL function with p pieces deﬁned on Rn. Then f can be written as f(x) = X S⊆[p], |S|≤n+1 cS max{aT i x + bi | i ∈S} with coeﬃcients cS ∈Z. For the convex case, this yields a stronger version of Theorem 1.3, stating that any (not nec- essarily convex) CPWL function can be written as a linear combination of (n + 1)-term maxima. Theorem 4.1 is stronger in the sense that it guarantees that all pieces of the (n + 1)-term maxima must be pieces of the original function. This makes it possible to bound the total number of these (n + 1)-term maxima and, therefore, the size of an NN representing f, as we will see in the proof of the following theorem. Theorem 4.2. Let f : Rn →R be a convex CPWL function with p aﬃne pieces. Then f can be represented by a ReLU NN with depth ⌈log2(n + 1)⌉+ 1 and width O(pn+1). Proof. Using the representation of Theorem 4.1, we can construct an NN computing f by computing all the (n + 1)-term max functions in parallel with the construction of Lemma 1.2 (similar to the proof by Arora et al. [6] to show Theorem 1.1). This results in an NN with the claimed depth. 22 Moreover, the width is at most a constant times the number of these (n + 1)-term max functions. This number can be bounded in terms of the number of possible subsets S ⊆[p] with |S| ≤n + 1, which is at most pn+1. Before we present the proof of Theorem 4.1, we show how we can generalize its consequences to the nonconvex case. 4.2 The General (Nonconvex) Case It is a well-known fact that every CPWL function can be expressed as a diﬀerence of two convex CPWL functions, see, e.g., [73, Theorem 1]. This allows us to derive the general case from the convex case. What we need, however, is to bound the number of aﬃne pieces of the two convex CPWL functions in terms of the number of pieces of the original function. Therefore, we consider a speciﬁc decomposition for which such bounds can easily be achieved. Proposition 4.3. Let f : Rn →R be a CPWL function with p aﬃne pieces. Then, one can write f as f = g −h where both g and h are convex CPWL functions with at most p2n+1 pieces. Proof. Suppose the p aﬃne pieces of f are given by x 7→aT i x + bi, i ∈[p]. Deﬁne the function h(x) := P 1≤i<j≤p max{aT i x + bi, aT j x + bj} and let g := f + h. Then, obviously, f = g −h. It remains to show that both g and h are convex CPWL functions with at most p2n+1 pieces. The convexity of h is clear by deﬁnition. Consider the \u0000p 2 \u0001 = p(p−1) 2 < p2 hyperplanes given by aT i x + bi = aT j x + bj, 1 ≤i < j ≤p. They divide Rn into at most \u0000p2 n \u0001 + \u0000 p2 n−1 \u0001 + · · · + \u0000p2 0 \u0001 ≤p2n regions (compare [20, Theorem 1.3]) in each of which h is aﬃne. In particular, h has at most p2n ≤p2n+1 pieces. Next, we show that g = f + h is convex. Intuitively, this holds because each possible breaking hyperplane of f is made convex by adding h. To make this formal, note that by the deﬁnition of convexity, it suﬃces to show that g is convex along each aﬃne line. For this purpose, consider an arbitrary line x(t) = ta + b, t ∈R, given by a ∈Rn and b ∈R. Let ˜f(t) := f(x(t)), ˜g(t) := g(x(t)), and ˜h(t) := h(x(t)). We need to show that ˜g: R →R is a convex function. Observe that ˜f, ˜g, and ˜h are clearly one-dimensional CPWL functions with the property ˜g = ˜f +˜h. Hence, it suﬃces to show that ˜g is locally convex around each of its breakpoints. Let t ∈R be an arbitrary breakpoint of ˜g. If ˜f is already locally convex around t, then the same holds for ˜g as well since ˜h inherits convexity from h. Now suppose that t is a nonconvex breakpoint of ˜f. Then there exist two distinct pieces of f, indexed by i, j ∈[p] with i ̸= j, such that ˜f(t′) = min{aT i x(t′) + bi, aT j x(t′) + bj} for all t′ suﬃciently close to t. By construction, ˜h(t′) contains the summand max{aT i x(t′)+bi, aT j x(t′)+bj}. Thus, adding this summand to ˜f linearizes the nonconvex breakpoint of ˜f, while adding all the other summands preserves convexity. In total, ˜g is locally convex around t, which ﬁnishes the proof that g is a convex function. Finally, observe that pieces of g = f + h are always intersections of pieces of f and h, for which we have only p · p2n = p2n+1 possibilities. Having this, we may conclude the following. Theorem 1.9. Let f : Rn →R be a CPWL function with p aﬃne pieces. Then f can be represented by a ReLU NN with depth ⌈log2(n + 1)⌉+ 1 and width O(p2n2+3n+1). 23 Proof. Consider the decomposition f = g −h from Proposition 4.3. Using Theorem 4.2, we obtain that both g and h can be represented with the required depth ⌈log2(n + 1)⌉+ 1 and with width O((p2n+1)n+1) = O(p2n2+3n+1). Thus, the same holds true for f. 4.3 Extended Newton Polyhedra of Convex CPWL Functions For our proof of Theorem 4.1, we use a correspondence of convex CPWL functions with certain polyhedra, which are known as (extended) Newton polyhedra in tropical geometry [49]. These relations between tropical geometry and neural networks have previously been applied to investigate expressivity of NNs; compare our references in Section 1.5. In order to formalize this correspondence, let CCPWLn ⊆CPWLn be the set of convex CPWL functions of type Rn →R. For f(x) = max{aT i x + bi | i ∈[p]} in CCPWLn, we deﬁne its so-called extended Newton polyhedron to be N(f) := conv({(aT i , bi)T ∈Rn × R | i ∈[p]}) + cone({−en+1}) ⊆Rn+1, where the “+” stands for Minkowski addition. We denote the set of all possible extended Newton polyhedra in Rn+1 as Newtn. That is, Newtn is the set of (unbounded) polyhedra in Rn+1 that emerge from a polytope by adding the negative of the (n + 1)-st unit vector −en+1 as an extreme ray. Hence, a set P ⊆Rn+1 is an element of Newtn if and only if P can be written as P = conv({(aT i , bi)T ∈Rn × R | i ∈[p]}) + cone({−en+1}). Conversely, for a polyhedron P ∈Newtn of this form, let F(P) ∈CCPWLn be the function deﬁned by F(P)(x) = max{aT i x + bi | i ∈[p]}. There is an intuitive way of thinking about the extended Newton polyhedron P of a convex CPWL function f: it consists of all hyperplane coeﬃcients (aT , b)T ∈Rn × R such that aT x + b ≤ f(x) for all x ∈Rn. This also explains why we add the extreme ray −en+1: decreasing b obviously maintains the property of aT x+b being a lower bound on the function f. Hence, if a point (aT , b)T belongs to the extended Newton polyhedron P, then also all points (aT , b′)T with b′ < b should belong to it. Thus, −en+1 should be contained in the recession cone of P. In fact, there is a one-to-one correspondence between elements of CCPWLn and Newtn, which is nicely compatible with some (functional and polyhedral) operations. This correspondence has been studied before in tropical geometry [41,49], convex geometry2 [39], as well as neural network literature [2,15,54,76]. We summarize the key ﬁndings about this correspondence relevant to our work in the following proposition: Proposition 4.4. Let n ∈N and f1, f2 ∈CCPWLn. Then it holds that (i) the functions N : CCPWLn →Newtn and F : Newtn →CCPWLn are well-deﬁned, that is, their output is independent from the representation of the input by pieces or vertices, respectively, (ii) N and F are bijections and inverse to each other, (iii) N(max{f1, f2}) = conv(N(f1), N(f2)) := conv(N(f1) ∪N(f2)), 2N(f) is the negative of the epigraph of the convex conjugate of f. 24 (iv) N(f1 + f2) = N(f1) + N(f2), where the + on the right-hand side is Minkowski addition. An algebraic way of phrasing this proposition is as follows: N and F are isomorphisms between the semirings (CCPWLn, max, +) and (Newtn, conv, +). 4.4 Proof of Theorem 4.1 The rough idea to prove Theorem 4.1 is as follows. Suppose we have a p-term max function f with p ≥n + 2. By Proposition 4.4, f corresponds to a polyhedron P ∈Newtn with at least n + 2 vertices. Applying a classical result from discrete geometry known as Radon’s theorem allows us to carefully decompose P into a “signed”3 Minkowski sum of polyhedra in Newtn whose vertices are subsets of at most p −1 out of the p vertices of P. Translating this back into the world of CPWL functions by Proposition 4.4 yields that f can be written as linear combination of p′-term maxima with p′ < p, where each of them involves a subset of the p aﬃne terms of f. We can then obtain Theorem 4.1 by iterating until every occurring maximum expression involves at most n + 1 terms. We start with a proposition that will be useful for our proof of Theorem 4.1. Although its statement is well-known in the discrete geometry community, we include a proof for the sake of completeness. To show the proposition, we make use of Radon’s theorem (compare [20, Theo- rem 4.1]), stating that any set of at least n + 2 points in Rn can be partitioned into two nonempty subsets such that their convex hulls intersect. Proposition 4.5. Given p > n + 1 vectors zi = (aT i , bi)T ∈Rn+1, i ∈[p], there exists a nonempty subset U ⊊[p] featuring the following property: there is no c ∈Rn+1 with cn+1 ≥0 and γ ∈R such that cT zi > γ for all i ∈U, and cT zi ≤γ for all i ∈[p] \\ U. (8) Proof. Radon’s theorem applied to the at least n + 2 vectors ai, i ∈[p], yields a nonempty sub- set U ⊊[p] and coeﬃcients λi ∈[0, 1] with P i∈U λi = P i∈[p]\\U λi = 1 such that P i∈U λiai = P i∈[p]\\U λiai. Suppose that P i∈U λibi ≤P i∈[p]\\U λibi without loss of generality (otherwise ex- change the roles of U and [p] \\ U). For any c and γ that satisfy (8) and cn+1 ≥0 it follows that γ < cT X i∈U λizi ≤cT X i∈[p]\\U λizi ≤γ, proving that no such c and γ can exist. The following proposition is a crucial step in order to show that any convex CPWL function with p > n + 1 pieces can be expressed as an integer linear combination of convex CPWL functions with at most p −1 pieces. 3Some polyhedra may occur with “negative” coeﬃcents in that sum, meaning that they are actually added to P instead of the other polyhedra. The corresponding CPWL functions will then have negative coeﬃcients in the linear combination representing f. 25 Proposition 4.6. Let f(x) = max{aT i x + bi | i ∈[p]} be a convex CPWL function deﬁned on Rn with p > n + 1. Then there exist a subset U ⊆[p] such that X W ⊆U, |W | even max{aT i x + bi | i ∈[p] \\ W} = X W ⊆U, |W | odd max{aT i x + bi | i ∈[p] \\ W} (9) Proof. Consider the p > n + 1 vectors zi := (aT i , bi)T ∈Rn+1, i ∈[p]. Choose U according to Proposition 4.5. We show that this choice of U guarantees equation (9). For W ⊆U, let fW(x) = max{aT i x + bi | i ∈[p] \\ W} and consider its extended Newton polyhedron PW = N(fW) = conv({zi | i ∈[p] \\ W}) + cone({−en+1}). By Proposition 4.4, equation (9) is equivalent to Peven := X W ⊆U, |W | even PW = X W ⊆U, |W | odd PW =: Podd, where the sums are Minkowski sums. We show this equation by showing that for all vectors c ∈Rn+1 it holds that max{cT x | x ∈Peven} = max{cT x | x ∈Podd}. (10) Let c ∈Rn+1 be an arbitrary vector. If cn+1 < 0, both sides of (10) are inﬁnite. Hence, from now on, assume that cn+1 ≥0. Then, both sides of (10) are ﬁnite since −en+1 is the only extreme ray of all involved polyhedra. Due to our choice of U according to Proposition 4.5, there exists an index u ∈U such that cT zu ≤max i∈[p]\\U cT zi. (11) We deﬁne a bijection ϕu between the even and the odd subsets of U as follows: ϕu(W) := \u001a W ∪{u}, if u /∈W, W \\ {u}, if u ∈W. That is, ϕu changes the parity of W by adding or removing u. Considering the corresponding polyhedra PW and Pϕu(W ), this means that ϕu adds or removes the extreme point zu to or from PW . Due to (11) this does not change the optimal value of maximizing in c-direction over the polyhedra, that is, max{cT x | x ∈PW} = max{cT x | x ∈Pϕu(W )}. Hence, we may conclude max{cT x | x ∈Peven} = X W ⊆U, |W | even max{cT x | x ∈PW } = X W ⊆U, |W | even max{cT x | x ∈Pϕu(W )} = X W ⊆U, |W | odd max{cT x | x ∈PW } = max{cT x | x ∈Podd}, which proves (10). Thus, the claim follows. 26 With the help of this result, we can now prove Theorem 4.1. Proof of Theorem 4.1. Let f(x) = max{aT i x + bi | i ∈[p]} be a convex CPWL function deﬁned on Rn. Having a closer look at the statement of Proposition 4.6, observe that only one term at the left-hand side of (9) contains all p aﬃne combinations aT i x + bi. Putting all other maximum terms on the other side, we may write f as an integer linear combination of maxima of at most p −1 summands. Repeating this procedure until we have eliminated all maximum terms with more than n + 1 summands yields the desired representation. 4.5 Potential Approaches to Show Lower Bounds on the Width In light of the upper width bounds shown in this section, a natural question to ask is whether also meaningful lower bounds can be achieved. This would mean constructing a family of CPWL functions with p pieces deﬁned on Rn (with diﬀerent values of p and n), for which we can prove that a large width is required to represent these functions with NNs of depth ⌈log2(n + 1)⌉+ 1. A trivial and not very satisfying answer follows, e.g., from [61] or [67]: for ﬁxed input dimension n, they show that a function computed by an NN with k hidden layers and width w has at most O(wkn) pieces. For our setting, this means that an NN with logarithmic depth needs a width of at least O(p1/(n log n)) to represent a function with p pieces. This is, of course, very far away from our upper bounds. Similar upper bounds on the number of pieces have been proven by many other authors and are often used to show depth-width trade-oﬀs [6, 54, 55, 59, 70]. However, there is a good reason why all these results only give rise to very trivial lower bounds for our setting: the focus is always on functions with considerably many pieces, which then, consequently, need many neurons to be represented (with small depth). However, since the lower bounds we strive for depend on the number of pieces, we would need to construct a family of functions with comparably few pieces that still need a lot of neurons to be represented. In general, it seems to be a tough task to argue why such functions should exist. A diﬀerent approach could leverage methods from complexity theory, in particular from circuit complexity. Neural networks are basically arithmetic circuits with very special operations allowed. In fact, they can be seen as a tropical variant of arithmetic circuits. Showing circuit lower bounds is a notoriously diﬃcult task in complexity theory, but maybe some conditional result (based on common conjectures similar to P ̸= NP) could be established. We think that the question whether our bounds are tight, or whether at least some non-trivial lower bounds on the width for NNs with logarithmic depth can be shown, is an exciting question for further research. 5 Understanding Expressivity via Newton Polytopes In Section 2, we presented a mixed-integer programming approach towards proving that deep NNs can strictly represent more functions than shallow ones. However, even if we could prove that it is indeed enough to consider H-conforming NNs, this approach would not generalize to deeper networks due to computational limitations. Therefore, diﬀerent ideas are needed to prove Conjec- ture 1.4 in its full generality. In this section, we point out that Newton polytopes of convex CPWL 27 functions (similar to what we used in the previous section) could also be a way of proving Conjec- ture 1.4. Using a homogenized version of Proposition 4.4, we provide an equivalent formulation of Conjecture 1.4 that is completely phrased in the language of discrete geometry. Recall that, by Proposition 2.3, we may restrict ourselves to NNs without biases. In particular, all CPWL functions represented by such NNs, or parts of it, are positively homogeneous. For the associated extended Newton polyhedra (compare Proposition 4.4), this has the following conse- quence: all vertices (a, b) ∈Rn × R lie in the hyperplane b = 0, that is, their (n + 1)-st coordinate is 0. Therefore, the extended Newton polyhedron of a positively homogeneous, convex CPWL function f(x) = max{aT i x | i ∈[p]} is completely characterized by the so-called Newton polytope, that is, the polytope conv({ai | i ∈[p]}) ⊆Rn. To make this formal, let CCPWLn be the set of all positively homogeneous, convex CPWL functions of type Rn →R and let Newtn be the set of all convex polytopes in Rn. Moreover, for f(x) = max{aT i x | i ∈[p]} in CCPWLn, let N (f) := conv({ai | i ∈[p]}) ∈Newtn be the associated Newton polytope of f and for P = conv({ai | i ∈[p]}) ∈Newtn let F(P)(x) = max{aT i x | i ∈[p]} be the so-called associated support function [38] of P in CCPWLn. With this notation, we obtain the following variant of Proposition 4.4. Proposition 5.1. Let n ∈N and f1, f2 ∈CCPWLn. Then it holds that (i) the functions N : CCPWLn →Newtn and F : Newtn →CCPWLn are well-deﬁned, that is, their output is independent from the representation of the input by pieces or vertices, respectively, (ii) N and F are bijections and inverse to each other, (iii) N(max{f1, f2}) = conv(N(f1), N(f2)) := conv(N(f1) ∪N (f2)), (iv) N(f1 + f2) = N(f1) + N(f2), where the + on the right-hand side is Minkowski addition. In other words, N and F are isomorphisms between the semirings (CCPWLn, max, +) and (Newtn, conv, +). Next, we study which polytopes can appear as Newton polytopes of convex CPWL functions computed by NNs with a certain depth; compare Zhang et al. [76]. Before we apply the ﬁrst ReLU activation, any function computed by an NN is linear. Thus, the corresponding Newton polytope is a single point. Starting from that, let us investigate a neuron in the ﬁrst hidden layer. Here, the ReLU activation function computes a maximum of a linear function and 0. Therefore, the Newton polytope of the resulting function is the convex hull of two points, that is, a line segment. After the ﬁrst hidden layer, arbitrary many functions of this type can be added up. For the corresponding Newton polytopes, this means that we take the Minkowski sum of line segments, resulting in a so-called zonotope. Now, this construction can be repeated layerwise, making use of Proposition 5.1: in each hidden layer, we can compute the maximum of two functions computed by the previous layers, which translates to obtaining the new Newton polytope as a convex hull of the union of the two original 28 x1 x2 y Newt(0) n points line segments Newt(1) n zonotopes conv(two zonotopes) Newt(2) n Figure 5: Set of polytopes that can arise as Newton polytopes of convex CPWL functions computed by (parts of) a 2-hidden-layer NN. Newton polytopes. In addition, the linear combinations between layers translate to scaling and taking Minkowski sums of Newton polytopes. This intuition motivates the following deﬁnition. Let Newt(0) n be the set of all polytopes in Rn that consist only of a single point. Then, for each k ≥1, we recursively deﬁne Newt(k) n := ( p X i=1 conv(Pi, Qi) \f\f\f\f\f Pi, Qi ∈Newt(k−1) n , p ∈N ) , where the sum is a Minkowski sum of polytopes. A ﬁrst, but not precisely accurate interpretation is as follows: the set Newt(k) n contains the Newton polytopes of positively homogeneous, convex CPWL functions representable with a k-hidden-layer NN. See Figure 5 for an illustration of the case k = 2. Unfortunately, this interpretation is not accurate for the following reason: our NNs are allowed to have negative weights, which cannot be fully captured by Minkowski sums as introduced above. Therefore, it might be possible that a k-hidden-layer NN can compute a convex function with Newton polytope not in Newt(k) n . Luckily, one can remedy this shortcoming, and even extend the interpretation to the non-convex case, by representing the computed function as diﬀerence of two convex functions. Theorem 5.2. A positively homogeneous (not necessarily convex) CPWL function can be com- puted by a k-hidden-layer NN if and only if it can be written as the diﬀerence of two positively homogeneous, convex CPWL functions with Newton polytopes in Newt(k) n . Proof. We use induction on k. For k = 0, the statement is clear since it holds precisely for linear functions. For the induction step, suppose that, for some k ≥1, the equivalence is valid up to k −1 hidden layers. We prove that it is also valid for k hidden layers. We need to show two directions. For the ﬁrst direction, assume that f is an arbitrary, positively homogeneous CPWL function that can be written as f = g −h with N (g), N(h) ∈Newt(k) n . We need to show that a k-hidden-layer NN can compute f. We show that this is even true for g and h, and hence, also for f. By deﬁnition of Newt(k) n , there exist a ﬁnite number p ∈N and polytopes Pi, Qi ∈Newt(k−1) n , i ∈[p], such that N(g) = Pp i=1 conv(Pi, Qi). By Proposition 5.1, 29 we have g = Pp i=1 max{F(Pi), F(Qi)}. By induction, F(Pi) and F(Qi) can be computed by NNs with k −1 hidden layers. Since the maximum terms can be computed with a single hidden layer, in total a k-th hidden layer is suﬃcient to compute g. An analogous argument applies to h. Thus, f is computable with k hidden layers, completing the ﬁrst direction. For the other direction, suppose that f is an arbitrary, positively homogeneous CPWL function that can be computed by a k-hidden-layer NN. Let us separately consider the nk neurons in the k-th hidden layer of the NN. Let ai, i ∈[nk], be the weight of the connection from the i-th neuron in that layer to the output. Without loss of generality, we have ai ∈{±1}, because otherwise we can normalize it and multiply the weights of the incoming connections to the i-th neuron with |ai| instead. Moreover, let us assume that, by potential reordering, there is some m ≤nk such that ai = 1 for i ≤m and ai = −1 for i > m. With these assumptions, we can write f = m X i=1 max{0, fi} − nk X i=m+1 max{0, fi}, (12) where each fi is computable by a (k−1)-hidden-layer NN, namely the sub-NN computing the input to the i-th neuron in the k-th hidden layer. By induction, we obtain fi = gi −hi for some positively homogeneous, convex functions gi, hi with N(gi), N (hi) ∈Newt(k−1) n . We then have max{0, fi} = max{gi, hi} −hi. (13) We deﬁne g := m X i=1 max{gi, hi} + nk X i=m+1 hi and h := m X i=1 hi + nk X i=m+1 max{gi, hi}. Note that g and h are convex by construction as a sum of convex functions and that (12) and (13) imply f = g −h. Moreover, by Proposition 5.1, N(g) = m X i=1 conv(N (gi), N(hi)) + nk X i=m+1 conv(N(hi), N (hi)) ∈Newt(k) n and N (h) = m X i=1 conv(N(hi), N (hi)) + nk X i=m+1 conv(N(gi), N(hi)) ∈Newt(k) n . Hence, f can be represented as desired, completing also the other direction. The power of Theorem 5.2 lies in the fact that it provides a purely geometric characterization of the class ReLU(k). The classes of polytopes Newt(k) n are solely deﬁned by the two simple geometric operations Minkowski sum and convex hull of the union. Therefore, understanding the class ReLU(k) is equivalent to understanding what polytopes one can generate by iterative application of these geometric operations. In particular, we can give yet another equivalent reformulation of our main conjecture. To this end, let the simplex ∆n := conv{0, e1, . . . , en} ⊆Rn denote the Newton polytope of the function fn = max{0, x1, . . . , xn} for each n ∈N. 30 Conjecture 5.3. For every k ∈N, n = 2k, there does not exist a pair of polytopes P, Q ∈Newt(k) n with ∆n + Q = P (Minkowski sum). Theorem 5.4. Conjecture 5.3 is equivalent to Conjecture 1.4 and Conjecture 1.5. Proof. By Proposition 1.6, it suﬃces to show equivalence between Conjecture 5.3 and Conjec- ture 1.5. By Theorem 5.2, fn can be represented with k hidden layers if and only if there are functions g and h with Newton polytopes in Newt(k) n satisfying fn + h = g. By Proposition 5.1, this happens if and only if there are polytopes P, Q ∈Newt(k) n with ∆n + Q = P. It is particularly interesting to look at special cases with small k. For k = 1, the set Newt(1) n is the set of all zonotopes. Hence, the (known) statement that max{0, x1, x2} cannot be computed with one hidden layer [56] is equivalent to the fact that the Minkowski sum of a zonotope and a triangle can never be a zonotope. The ﬁrst open case is the case k = 2. An unconditional proof that two hidden layers do not suﬃce to compute the maximum of ﬁve numbers is highly desired. In the regime of Newton polytopes, this means to understand the class Newt(2) n . It consists of ﬁnite Minkowski sums of polytopes that arise as the convex hull of the union of two zonotopes. Hence, the major open question here is to classify this set of polytopes. Finally, let us remark that there exists a generalization of the concept of polytopes, known as virtual polytopes [58], that makes it possible to assign a Newton polytope also to non-convex CPWL functions. This makes use of the fact that every (non-convex) CPWL function is a diﬀerence of two convex ones. Consequently, a virtual polytope is a formal Minkowski diﬀerence of two ordinary polytopes. Using this concept, Theorem 5.2 and Conjecture 5.3 can be phrased in a simpler way, replacing the pair of polytopes with a single virtual polytope. 6 Future Research The most obvious and, at the same time, most exciting open research question is to prove or disprove Conjecture 1.4, or equivalently Conjecture 1.5 or Conjecture 5.3. The ﬁrst step could be to prove that it is indeed enough to consider H-conforming NNs. This is intuitive because every breakpoint introduced at any place outside the hyperplanes Hij needs to be canceled out later. Therefore, it is natural to assume that these breakpoints do not have to be introduced in the ﬁrst place. However, this intuition does not seem to be enough for a formal proof because it could occur that additional breakpoints in intermediate steps, which are canceled out later, also inﬂuence the behavior of the function at other places where we allow breakpoints in the end. Another step towards resolving our conjecture may be to ﬁnd an alternative proof of Theo- rem 1.7, not using H-conforming NNs. This might also be beneﬁcial for generalizing our tech- niques to more hidden layers, since, while theoretically possible, a direct generalization of the MIP approach is infeasible due to computational limitations. For example, it might be particularly promising to use a tropical approach as described in Section 5 and apply methods from polytope theory to prove Conjecture 5.3. In light of our results from Section 3, it would be desirable to provide a complete character- ization of the functions contained in ReLU(k). Another potential research goal is improving our upper bounds on the width from Section 4 and/or proving matching lower bounds as discussed in Section 4.5. 31 Some more interesting research directions are the following: • establishing or strengthening our results for special classes of NNs like recurrent neural net- works (RNNs) or convolutional neural networks (CNNs), • using exact representation results to show more drastic depth-width trade-oﬀs compared to existing results in the literature, • understanding how the class ReLU(k) changes when a polynomial upper bound is imposed on the width of the NN; see related work by Vardi et al. [72]. • understanding which CPWL functions one can (exactly) represent with polynomial size at all, without any restriction on the depth; see related work in the context of combinatorial optimization [36,37]. References [1] M. Abrahamsen, L. Kleist, and T. Miltzow. Training neural networks is ER-complete. Advances in Neural Information Processing Systems (NeurIPS), 34, 2021. [2] M. Alfarra, A. Bibi, H. Hammoud, M. Gaafar, and B. Ghanem. On the decision boundaries of neural networks: A tropical geometry perspective. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022. [3] A. M. Alvarez, Q. Louveaux, and L. Wehenkel. A machine learning-based approximation of strong branching. INFORMS Journal on Computing, 29(1):185–195, 2017. [4] R. Anderson, J. Huchette, W. Ma, C. Tjandraatmadja, and J. P. Vielma. Strong mixed- integer programming formulations for trained neural networks. Mathematical Programming, pages 1–37, 2020. [5] M. Anthony and P. L. Bartlett. Neural network learning: Theoretical foundations. Cambridge University Press, 1999. [6] R. Arora, A. Basu, P. Mianjy, and A. Mukherjee. Understanding deep neural networks with rectiﬁed linear units. In International Conference on Learning Representations, 2018. [7] R. Bagnara, P. M. Hill, and E. Zaﬀanella. The Parma Polyhedra Library: Toward a complete set of numerical abstractions for the analysis and veriﬁcation of hardware and software systems. Science of Computer Programming, 72(1–2):3–21, 2008. [8] A. R. Barron. Universal approximation bounds for superpositions of a sigmoidal function. IEEE Transactions on Information theory, 39(3):930–945, 1993. [9] A. R. Barron. Approximation and estimation bounds for artiﬁcial neural networks. Machine learning, 14(1):115–133, 1994. [10] Y. Bengio, A. Lodi, and A. Prouvost. Machine learning for combinatorial optimization: a methodological tour d’horizon. European Journal of Operational Research, 2020. 32 [11] D. Bertschinger, C. Hertrich, P. Jungeblut, T. Miltzow, and S. Weber. Training fully connected neural networks is ER-complete. arXiv:2204.01368, 2022. [12] D. Bienstock, G. Mu˜noz, and S. Pokutta. Principled deep neural network training through linear programming. arXiv:1810.03218, 2018. [13] P. Bonami, A. Lodi, and G. Zarpellon. Learning a classiﬁcation of mixed-integer quadratic programming problems. In International Conference on the Integration of Constraint Pro- gramming, Artiﬁcial Intelligence, and Operations Research, pages 595–604. Springer, 2018. [14] D. Boob, S. S. Dey, and G. Lan. Complexity of training relu neural network. Discrete Opti- mization, 44, 2022. [15] V. Charisopoulos and P. Maragos. A tropical approach to neural networks with piecewise linear activations. arXiv preprint arXiv:1805.08749, 2018. [16] K.-L. Chen, H. Garudadri, and B. D. Rao. Improved bounds on neural complexity for rep- resenting piecewise linear functions. In Advances in Neural Information Processing Systems, 2022. [17] S. Chen, A. R. Klivans, and R. Meka. Learning Deep ReLU Networks Is Fixed-Parameter Tractable. In N. K. Vishnoi, editor, 2021 IEEE 62nd Annual Symposium on Foundations of Computer Science (FOCS), pages 696–707, 2022. [18] G. Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of control, signals and systems, 2(4):303–314, 1989. [19] S. S. Dey, G. Wang, and Y. Xie. Approximation algorithms for training one-node relu neural networks. IEEE Transactions on Signal Processing, 68:6696–6706, 2020. [20] H. Edelsbrunner. Algorithms in Combinatorial Geometry. Springer Science & Business Media, 1987. [21] R. Eldan and O. Shamir. The power of depth for feedforward neural networks. In Conference on Learning Theory, pages 907–940, 2016. [22] M. Fischetti and J. Jo. Deep neural networks as 0-1 mixed integer linear programs: A feasibility study. arXiv preprint arXiv:1712.06174, 2017. [23] V. Froese, C. Hertrich, and R. Niedermeier. The computational complexity of ReLU network training parameterized by data dimensionality. Journal of Artiﬁcial Intelligence Research, 74:1775–1790, 2022. [24] M. Gasse, D. Ch´etelat, N. Ferroni, L. Charlin, and A. Lodi. Exact combinatorial optimization with graph convolutional neural networks. Advances in neural information processing systems, 32, 2019. [25] S. Goel, V. Kanade, A. Klivans, and J. Thaler. Reliably learning the relu in polynomial time. In Conference on Learning Theory, pages 1004–1042. PMLR, 2017. 33 [26] S. Goel, A. Klivans, and R. Meka. Learning one convolutional layer with overlapping patches. In International Conference on Machine Learning, pages 1783–1791. PMLR, 2018. [27] S. Goel and A. R. Klivans. Learning neural networks with two nonlinear layers in polynomial time. In Conference on Learning Theory, pages 1470–1499. PMLR, 2019. [28] S. Goel, A. R. Klivans, P. Manurangsi, and D. Reichman. Tight hardness results for training depth-2 ReLU networks. In 12th Innovations in Theoretical Computer Science Conference (ITCS ’21), volume 185 of LIPIcs, pages 22:1–22:14. Schloss Dagstuhl - Leibniz-Zentrum f¨ur Informatik, 2021. [29] R. Gribonval, G. Kutyniok, M. Nielsen, and F. Voigtlaender. Approximation spaces of deep neural networks. Constructive Approximation, pages 1–109, 2021. [30] Gurobi Optimization, LLC. Gurobi optimizer reference manual, 2021. [31] C. A. Haase, C. Hertrich, and G. Loho. Lower bounds on the depth of integral ReLU neu- ral networks via lattice polytopes. In The Eleventh International Conference on Learning Representations, 2023. [32] B. Hanin. Universal function approximation by deep neural nets with bounded width and ReLU activations. Mathematics, 7(10):992, 2019. [33] B. Hanin and M. Sellke. Approximating continuous functions by ReLU nets of minimal width. arXiv:1710.11278, 2017. [34] H. He, H. Daume III, and J. M. Eisner. Learning to search in branch and bound algorithms. Advances in neural information processing systems, 27:3293–3301, 2014. [35] J. He, L. Li, J. Xu, and C. Zheng. Relu deep neural networks and linear ﬁnite elements. Journal of Computational Mathematics, 38(3):502–527, 2020. [36] C. Hertrich and L. Sering. ReLU neural networks of polynomial size for exact maximum ﬂow computation. In International Conference on Integer Programming and Combinatorial Optimization, 2023. [37] C. Hertrich and M. Skutella. Provably good solutions to the knapsack problem via neural networks of bounded size. In AAAI Conference on Artiﬁcial Intelligence, 2021. [38] J.-B. Hiriart-Urruty and C. Lemar´echal. Convex analysis and minimization algorithms I, volume 305 of Grundlehren der mathematischen Wissenschaften. Springer-Verlag, Berlin, 1993. [39] J.-B. Hiriart-Urruty and C. Lemar´echal. Convex Analysis and Minimization Algorithms II, volume 306 of Grundlehren der mathematischen Wissenschaften. Springer-Verlag, Berlin, 1993. [40] K. Hornik. Approximation capabilities of multilayer feedforward networks. Neural networks, 4(2):251–257, 1991. [41] M. Joswig. Essentials of tropical combinatorics. Graduate Studies in Mathematics. American Mathematical Society, Providence, RI, 2022. To appear. 34 [42] S. Khalife and A. Basu. Neural networks with linear threshold activations: structure and algo- rithms. In International Conference on Integer Programming and Combinatorial Optimization, pages 347–360. Springer, 2022. [43] E. Khalil, P. Le Bodic, L. Song, G. Nemhauser, and B. Dilkina. Learning to branch in mixed integer programming. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 30, 2016. [44] E. B. Khalil, B. Dilkina, G. L. Nemhauser, S. Ahmed, and Y. Shao. Learning to run heuristics in tree search. In IJCAI, pages 659–666, 2017. [45] M. Kruber, M. E. L¨ubbecke, and A. Parmentier. Learning when to use a decomposition. In International Conference on AI and OR Techniques in Constraint Programming for Combi- natorial Optimization Problems, pages 202–210. Springer, 2017. [46] S. Liang and R. Srikant. Why deep neural networks for function approximation? In Interna- tional Conference on Learning Representations, 2017. [47] A. Lodi and G. Zarpellon. On learning and branching: a survey. TOP, 25(2):207–236, 2017. [48] Z. Lu. A note on the representation power of GHHs. arXiv:2101.11286, 2021. [49] D. Maclagan and B. Sturmfels. Introduction to tropical geometry, volume 161 of Graduate Studies in Mathematics. American Mathematical Soc., 2015. [50] P. Maragos, V. Charisopoulos, and E. Theodosis. Tropical geometry and machine learning. Proceedings of the IEEE, 109(5):728–755, 2021. [51] H. Mhaskar. Approximation of real functions using neural networks. In Proc. Intl. Conf. Comp. Math., New Delhi, India, World Scientiﬁc Press, pages 267–278. World Scientiﬁc, 1993. [52] H. N. Mhaskar. Neural networks for optimal approximation of smooth and analytic functions. Neural computation, 8(1):164–177, 1996. [53] H. N. Mhaskar and C. A. Micchelli. Degree of approximation by neural and translation net- works with a single hidden layer. Advances in applied mathematics, 16(2):151–183, 1995. [54] G. Mont´ufar, Y. Ren, and L. Zhang. Sharp bounds for the number of regions of maxout networks and vertices of minkowski sums. SIAM Journal on Applied Algebra and Geometry, 6(4):618–649, 2022. [55] G. F. Mont´ufar, R. Pascanu, K. Cho, and Y. Bengio. On the number of linear regions of deep neural networks. In Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 27, pages 2924–2932. 2014. [56] A. Mukherjee and A. Basu. Lower bounds over boolean inputs for deep neural networks with ReLU gates. arXiv:1711.03073, 2017. [57] Q. Nguyen, M. C. Mukkamala, and M. Hein. Neural networks should be wide enough to learn disconnected decision regions. In International Conference on Machine Learning, pages 3737–3746, 2018. 35 [58] G. Y. Panina and I. Stre˘ınu. Virtual polytopes. Uspekhi Mat. Nauk, 70(6(426)):139–202, 2015. [59] R. Pascanu, G. Mont´ufar, and Y. Bengio. On the number of inference regions of deep feed forward networks with piece-wise linear activations. In International Conference on Learning Representations, 2014. [60] A. Pinkus. Approximation theory of the mlp model. Acta Numerica 1999: Volume 8, 8:143– 195, 1999. [61] M. Raghu, B. Poole, J. Kleinberg, S. Ganguli, and J. S. Dickstein. On the expressive power of deep neural networks. In International Conference on Machine Learning, pages 2847–2854, 2017. [62] F. Rosenblatt. The perceptron: a probabilistic model for information storage and organization in the brain. Psychological review, 65(6):386, 1958. [63] I. Safran and O. Shamir. Depth-width tradeoﬀs in approximating natural functions with neural networks. In International Conference on Machine Learning, pages 2979–2987, 2017. [64] A. Schrijver. Theory of Linear and Integer Programming. John Wiley and Sons, New York, 1986. [65] T. Serra, A. Kumar, and S. Ramalingam. Lossless compression of deep neural networks. In International Conference on Integration of Constraint Programming, Artiﬁcial Intelligence, and Operations Research, pages 417–430. Springer, 2020. [66] T. Serra and S. Ramalingam. Empirical bounds on linear regions of deep rectiﬁer networks. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 34, pages 5628–5635, 2020. [67] T. Serra, C. Tjandraatmadja, and S. Ramalingam. Bounding and counting linear regions of deep neural networks. In International Conference on Machine Learning, pages 4565–4573, 2018. [68] R. P. Stanley. An introduction to hyperplane arrangements. In Lecture notes, IAS/Park City Mathematics Institute, 2004. [69] M. Telgarsky. Representation beneﬁts of deep feedforward networks. arXiv:1509.08101, 2015. [70] M. Telgarsky. Beneﬁts of depth in neural networks. In Conference on Learning Theory, pages 1517–1539, 2016. [71] The Sage Developers. SageMath, the Sage Mathematics Software System (Version 9.0), 2020. https://www.sagemath.org. [72] G. Vardi, D. Reichman, T. Pitassi, and O. Shamir. Size and depth separation in approximating benign functions with neural networks. In Conference on Learning Theory, pages 4195–4223. PMLR, 2021. [73] S. Wang. General constructive representations for continuous piecewise-linear functions. IEEE Transactions on Circuits and Systems I: Regular Papers, 51(9):1889–1896, 2004. 36 [74] S. Wang and X. Sun. Generalization of hinging hyperplanes. IEEE Transactions on Informa- tion Theory, 51(12):4425–4431, 2005. [75] D. Yarotsky. Error bounds for approximations with deep relu networks. Neural Networks, 94:103–114, 2017. [76] L. Zhang, G. Naitzat, and L.-H. Lim. Tropical geometry of deep neural networks. In Interna- tional Conference on Machine Learning, pages 5819–5827, 2018. 37 \n"
     ]
    }
   ],
   "source": [
    "print(paper)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "InfoFeeder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
